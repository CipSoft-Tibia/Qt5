{"version":3,"file":"webidl2.js","mappings":"CAAA,SAA2CA,EAAMC,GAC1B,iBAAZC,SAA0C,iBAAXC,OACxCA,OAAOD,QAAUD,IACQ,mBAAXG,QAAyBA,OAAOC,IAC9CD,OAAO,GAAIH,GACe,iBAAZC,QACdA,QAAiB,QAAID,IAErBD,EAAc,QAAIC,IARpB,CASGK,YAAY,I,mBCRf,IAAIC,EAAsB,CCA1B,EAAwB,CAACL,EAASM,KACjC,IAAI,IAAIC,KAAOD,EACXD,EAAoBG,EAAEF,EAAYC,KAASF,EAAoBG,EAAER,EAASO,IAC5EE,OAAOC,eAAeV,EAASO,EAAK,CAAEI,YAAY,EAAMC,IAAKN,EAAWC,MCJ3E,EAAwB,CAACM,EAAKC,IAAUL,OAAOM,UAAUC,eAAeC,KAAKJ,EAAKC,GCClF,EAAyBd,IACH,oBAAXkB,QAA0BA,OAAOC,aAC1CV,OAAOC,eAAeV,EAASkB,OAAOC,YAAa,CAAEC,MAAO,WAE7DX,OAAOC,eAAeV,EAAS,aAAc,CAAEoB,OAAO,M,KCoCvD,SAASC,EACPC,EACAC,EACAC,EACAC,EACAC,GACA,MAAEC,EAAQ,QAAO,QAAEC,EAAO,SAAEC,GAAa,IAKzC,SAASC,EAAYC,GACnB,OAAOA,EAAQ,EACXT,EAAOU,MAAMT,EAAUA,EAAWQ,GAClCT,EAAOU,MAAMC,KAAKC,IAAIX,EAAWQ,EAAO,GAAIR,GASlD,SAASY,EAAaC,GAAQ,SAAEC,GAAa,IAC3C,MAAMC,EAAOF,EAAOG,KAAKC,GAAMA,EAAEC,OAASD,EAAEpB,QAAOsB,KAAK,IAClDC,EAAYrB,EAAOC,GACzB,MAAuB,QAAnBoB,EAAUC,KACLN,EAELD,EACKC,EAAOK,EAAUF,OAEnBH,EAAKN,MAAMW,EAAUF,OAAOI,QAGrC,MACMC,EACsB,QAA1BxB,EAAOC,GAAUqB,KACbtB,EAAOC,GAAUuB,KACjBxB,EAAOuB,OAAS,EAChBvB,EAAOC,EAAW,GAAGuB,KACrB,EAEAC,EAjFR,SAAkBT,GAChB,MAAMU,EAAWV,EAAKW,MAAM,MAC5B,OAAOD,EAASA,EAASH,OAAS,GA+ERK,CACxBf,EAAaL,GATG,GASsB,CAAEO,UAAU,KAG9Cc,EAAmBrB,EAZP,GAaZsB,EAAiBjB,EAAagB,GAI9BE,EAAgBN,EAHMK,EAAeH,MAAM,MAAM,GAGS,MADjD,IAAIK,OAAOP,EAAkBF,QAAU,KAGhDU,EAAuB,WAAT7B,EAAoB,QAAU,SAQ5C8B,EAAU,GAAG9B,mBAAsBoB,IAPpBxB,EAAOmC,KAAO,OAAOnC,EAAOmC,OAAS,KAExDjC,GAAWA,EAAQiC,KACf,KAAKF,OAAiB/B,EAAQkC,QAAU,WAAa,KAnF7D,SAAuBC,GACrB,MAAMC,EAAY,CAACD,GACnB,KAAOA,GAAQA,EAAKE,QAAQ,CAC1B,MAAM,OAAEA,GAAWF,EACnBC,EAAUE,QAAQD,GAClBF,EAAOE,EAET,OAAOD,EAAUrB,KAAKwB,GAfxB,SAAuBC,EAAMC,GAC3B,IAAIC,EAASF,EAIb,OAHIC,IACFC,GAAU,IAAID,KAETC,EAUqBC,CAAcJ,EAAEnB,KAAMmB,EAAEN,QAAOf,KAAK,QA4EA0B,CACxD5C,OAEF,QACiF6B,IACvF,MAAO,CACL5B,QAAS,GAAG+B,KAAW/B,IACvB4C,YAAa5C,EACb+B,QAAAA,EACAV,KAAAA,EACAwB,WAAYhD,EAAOmC,KACnB9B,MAAAA,EACAE,SAAAA,EACAD,QAAAA,EACA2C,MAAOnB,EACPoB,OAAQrB,GAOL,SAASsB,EAAYnD,EAAQC,EAAUC,EAASC,GACrD,OAAOJ,EAAMC,EAAQC,EAAUC,EAASC,EAAS,UAO5C,SAASiD,EACdC,EACAnD,EACAK,EACAJ,EACAmD,EAAU,IAGV,OADAA,EAAQ/C,SAAWA,EACZR,EACLG,EAAQF,OACRqD,EAAME,MACNrD,EACAC,EACA,aACAmD,G,6FC/IG,MAAME,KAMXC,aAAY,OAAEzD,EAAM,OAAEkD,IACpB/D,OAAOuE,iBAAiBC,KAAM,CAC5B3D,OAAQ,CAAEF,MAAOE,GACjBkD,OAAQ,CAAEpD,MAAOoD,EAAQU,UAAU,GACnCrB,OAAQ,CAAEzC,MAAO,KAAM8D,UAAU,GACjCD,KAAM,CAAE7D,MAAO6D,QAInBE,SACE,MAAMC,EAAO,CAAExC,UAAMyC,EAAW5B,UAAM4B,EAAWC,iBAAaD,GAC9D,IAAIE,EAAQN,KACZ,KAAOM,IAAU9E,OAAOM,WAAW,CACjC,MAAMyE,EAAU/E,OAAOgF,0BAA0BF,GACjD,IAAK,MAAOhF,EAAKa,KAAUX,OAAOiF,QAAQF,IACpCpE,EAAMT,YAAcS,EAAMR,OAE5BwE,EAAK7E,GAAO0E,KAAK1E,IAGrBgF,EAAQ9E,OAAOkF,eAAeJ,GAEhC,OAAOH,GCnBJ,SAASQ,EACdC,EACAC,GACA,iBAAEC,GAAqB,IAEvB,IAAKF,EAAQG,MAAO,CAClB,MAAMC,EAAMH,EAAKI,OAAOtF,IAAIiF,EAAQA,SACpC,IAAKI,EACH,OAEF,GAAiB,YAAbA,EAAIrD,KAAoB,CAC1B,MAAM,0BAAEuD,GAA8BL,EAAKM,MAC3C,GAAID,EAA0BE,IAAIJ,GAGhC,OAAOE,EAA0BvF,IAAIqF,GAEvCH,EAAKM,MAAMD,0BAA0BG,IAAIL,OAAKZ,GAC9C,MAAMnB,EAAS0B,EAA0BK,EAAIJ,QAASC,GAEtD,GADAA,EAAKM,MAAMD,0BAA0BG,IAAIL,EAAK/B,GAC1CA,EACF,MAAO,CACLqC,UAAWV,EACXW,WAAYtC,EAAOsC,YAIzB,GAAiB,eAAbP,EAAIrD,OAA0BmD,IAAqBF,EAAQY,UAC7D,MAAO,CACLF,UAAWV,EACXW,WAAYP,GAIlB,IAAK,MAAMS,KAAWb,EAAQa,QAAS,CACrC,MAAMxC,EAAS0B,EAA0Bc,EAASZ,GAClD,GAAI5B,EACF,OAAIwC,EAAQV,MACH9B,EAEF,CACLqC,UAAWG,EACXF,WAAYtC,EAAOsC,aAWpB,SAASG,EAAgCC,EAAMd,GACpD,GAAIA,EAAKM,MAAMO,gCAAgCN,IAAIO,GACjD,OAAOd,EAAKM,MAAMO,gCAAgC/F,IAAIgG,GAIxDd,EAAKM,MAAMO,gCAAgCL,IAAIM,OAAMvB,GACrD,IAAInB,EAAS0C,EAAKC,QAAQC,MAAMC,GAAUA,EAAMC,WAChD,IAAK9C,GAAU0C,EAAKtB,YAAa,CAC/B,MAAM2B,EAAYnB,EAAKI,OAAOtF,IAAIgG,EAAKtB,aAClC2B,EAGMN,EAAgCM,EAAWnB,KACpD5B,GAAS,GAFTA,GAAS,EAMb,OADA4B,EAAKM,MAAMO,gCAAgCL,IAAIM,EAAM1C,GAC9CA,EChFF,MAAMgD,kBAAkBC,MAC7BpC,aAAY,OAAEzD,EAAM,OAAEkD,IACpB4C,QACA3G,OAAOuE,iBAAiBC,KAAM,CAC5B3D,OAAQ,CAAEF,MAAOE,GACjBkD,OAAQ,CAAEpD,MAAOoD,GACjBX,OAAQ,CAAEzC,MAAO,KAAM8D,UAAU,MCHhC,MAAMmC,qBAAqBvC,KAKhC,cAAcwC,EAAW1E,GACvB,MAAO,KACL,MAAMxB,EAAQkG,EAAUC,YAAY3E,GACpC,GAAIxB,EACF,OAAO,IAAIiG,aAAa,CACtB/F,OAAQgG,EAAUhG,OAClBkD,OAAQ,CAAEpD,MAAAA,MAMlB,YACE,OAAO,EAAS6D,KAAKT,OAAOpD,MAAMA,OAIpCoG,MAAMC,GACJ,OAAOA,EAAEC,GAAGC,KAAK,CACfF,EAAE9C,MAAMM,KAAKT,OAAOpD,OACpBqG,EAAE9C,MAAMM,KAAKT,OAAOoD,cAKnB,MAAMC,YAAYR,aAIvB,aAAaC,GACX,MAAMlG,EAAQkG,EAAUC,YAAY,OACpC,GAAInG,EACF,OAAO,IAAIyG,IAAI,CAAEvG,OAAQgG,EAAUhG,OAAQkD,OAAQ,CAAEpD,MAAAA,KAIzD,WACE,MAAO,OCnCX,SAASoD,EAAO8C,EAAWQ,GACzB,OAAOC,EAAKT,EAAW,CACrBU,OAAQX,aAAaW,OAAOV,EAAWQ,GACvCG,SAAUH,EAAY,UAI1B,MAAMI,EAAqB,CAAC,aAAc,UAAW,UAAW,UAU1DC,EAAkB,IAAIC,IAAI,IARD,CAC7B,oBACA,gBACA,cACA,uBACA,eAKyB7F,KAAKkB,GAAS,CAACA,EAAM,SAASA,OAEvD,CAAC,mBAAoB,yBACrB,CAAC,mBAAoB,0BACrB,CAAC,cAAe,6BAOlB,SAAS4E,EAAiBf,GACxB,IAAK,MAAMgB,KAAUJ,EAAoB,CACvC,MAAMK,EAAO/D,EAAO8C,EAAWgB,GAC/B,GAAIC,EAAK1F,OACP,OAAO0F,EAGXjB,EAAUjG,MACR,uEAIG,MAAMmH,oCAAoC1D,KAI/C,aAAawC,GACX,MAAM9C,EAAS,CAAEiE,OAAQnB,EAAUoB,QAAQ,MACrCC,EAAMC,EACV,IAAIJ,4BAA4B,CAAElH,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAG9D,GADAmE,EAAIZ,KAAO,GACPvD,EAAOiE,OAAQ,CAEjB,GADAjE,EAAOqE,SAAWvB,EAAUoB,QAAQ,KAChClE,EAAOqE,SACT,OAAOF,EAAI1D,KAEbT,EAAOsE,cAAgBxB,EAAUC,eAAeW,GAelD,OAbA1D,EAAOuE,KAAOzB,EAAUoB,QAAQ,KAC5BlE,EAAOuE,MACTJ,EAAIZ,KAAOY,EAAIK,UAEXX,EAAiBf,GAEjB2B,EAAc3B,GAClB9C,EAAO0E,MACL5B,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,yDACTmD,EAAOiE,SAAWjE,EAAOsE,eAClCxB,EAAUjG,MAAM,uDAEXsH,EAAI1D,KAGb,gBACE,OACEA,KAAKT,OAAOiE,SAAWxD,KAAKT,OAAOqE,WAAa5D,KAAKT,OAAOsE,cAIhE,cACE,OAAI7D,KAAK+D,UACA/D,KAAK8C,KAAK,GAAGvD,OAAOpD,MAAMwB,KAAO,QAEtCqC,KAAKT,OAAOqE,SACP,IAEL5D,KAAKT,OAAOsE,cACP7D,KAAKT,OAAOsE,cAAclG,KAE5B,KAIT4E,MAAMC,GACJ,MAAM,QAAE0B,GAAYlE,KACpB,OAAOwC,EAAEC,GAAGC,KAAK,CACfF,EAAE9C,MAAMM,KAAKT,OAAOiE,QACpBhB,EAAE9C,MAAMM,KAAKT,OAAOqE,UACpBpB,EAAE2B,gBAAgBnE,KAAKT,OAAOsE,cAAe7D,KAAKpB,QAClD4D,EAAE9C,MAAMM,KAAKT,OAAOuE,SACjB9D,KAAK8C,KAAKxF,KAAK8G,GACG,oBAAZF,EACH1B,EAAE6B,WAAWD,EAAGpE,KAAKpB,QACrBwF,EAAE7B,MAAMC,KAEdA,EAAE9C,MAAMM,KAAKT,OAAO0E,UAKnB,MAAMK,gCAAgCzE,KAI3C,aAAawC,GACX,MAAM7D,EAAO6D,EAAUC,YAAY,cACnC,GAAI9D,EACF,OAAO,IAAI8F,wBAAwB,CACjCjI,OAAQgG,EAAUhG,OAClBkD,OAAQ,CAAEf,KAAAA,GACV+F,OAAQhB,4BAA4BiB,MAAMnC,KAKhDvC,aAAY,OAAEzD,EAAM,OAAEkD,EAAM,OAAEgF,IAC5BpC,MAAM,CAAE9F,OAAAA,EAAQkD,OAAAA,IAChBgF,EAAO3F,OAASoB,KAChBxE,OAAOC,eAAeuE,KAAM,SAAU,CAAE7D,MAAOoI,IAGjD,WACE,MAAO,qBAET,WACE,OAAOvE,KAAKT,OAAOf,KAAKrC,MAE1B,UACE,MAAQ+H,QAASvG,EAAI,OAAE4B,EAAM,KAAEuD,GAAS9C,KAAKuE,OAC7C,IAAK5G,EACH,OAAO,KAOT,MAAO,CAAEA,KAAAA,EAAMxB,MALD6D,KAAKuE,OAAOR,UACtBjB,EACA9C,KAAKuE,OAAOhF,OAAOsE,cACnB,EAAStE,EAAOsE,cAAc1H,OAC9B,MAGN,gBACE,MAAM,UAAE4H,EAAS,KAAEjB,GAAS9C,KAAKuE,OACjC,OAAKzB,GAAQiB,EACJ,GAEFjB,EAGT,UAAUjC,GACR,MAAM,KAAErC,GAASwB,KACjB,GAAa,4BAATxB,EAAoC,CACtC,MAAMhC,EAAU,sOAIViD,EACJO,KAAKT,OAAOf,KACZwB,KACA,uBACAxD,EACA,CAAEE,MAAO,iBAEN,GAAIwG,EAAgB9B,IAAI5C,GAAO,CACpC,MAAMhC,EAAU,MAAMgC,yEACA0E,EAAgBvH,IAAI6C,oHAGpCiB,EAAgBO,KAAKT,OAAOf,KAAMwB,KAAM,iBAAkBxD,EAAS,CACvEE,MAAO,UACPC,SA0B+B8H,EA1BQzE,KA2BtC,KACL,MAAM,KAAExB,GAASiG,EACjBA,EAAQlF,OAAOf,KAAKrC,MAAQ+G,EAAgBvH,IAAI6C,GACnC,gBAATA,IACFiG,EAAQF,OAAOhF,OAAS,QAL9B,IAAuCkF,EAvBnC,IAAK,MAAMC,KAAO1E,KAAK2E,gBACdD,EAAIE,SAAS/D,GAKxB0B,MAAMC,GACJ,OAAOA,EAAEC,GAAGC,KAAK,CACfF,EAAEC,GAAGjF,OAAOwC,KAAKT,OAAOf,KAAKhB,QAC7BgF,EAAEC,GAAGoC,kBACHrC,EAAEC,GAAGC,KAAK,CACRF,EAAEC,GAAGqC,2BAA2B9E,KAAKxB,MACrCwB,KAAKuE,OAAOhC,MAAMC,MAGtBA,EAAE9C,MAAMM,KAAKT,OAAOoD,cAoBnB,MAAMoC,2BAA2B9C,UAItC,aAAaI,GACX,MAAM9C,EAAS,GACfA,EAAOuE,KAAOzB,EAAUoB,QAAQ,KAChC,MAAMC,EAAM,IAAIqB,mBAAmB,CAAE1I,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAC/D,OAAKA,EAAOuE,MACZJ,EAAIsB,QACClC,EAAKT,EAAW,CACjBU,OAAQuB,wBAAwBE,MAChCxB,SAAU,wBAGdzD,EAAO0E,MACL5B,EAAUoB,QAAQ,MAClBpB,EAAUjG,MACR,4DAECsH,EAAI9F,SACPyE,EAAU4C,UAAU1F,EAAO0E,MAAMrE,OACjCyC,EAAUjG,MAAM,iDAEdiG,EAAU6C,MAAM,MAClB7C,EAAUjG,MACR,kEAGGsH,GArBkBA,EAwB3B,UAAU7C,GACR,IAAK,MAAM4D,KAAWzE,WACbyE,EAAQG,SAAS/D,GAK5B0B,MAAMC,GACJ,OAAKxC,KAAKpC,OACH4E,EAAEC,GAAGC,KAAK,CACfF,EAAE9C,MAAMM,KAAKT,OAAOuE,SACjB9D,KAAK1C,KAAK6H,GAAOA,EAAG5C,MAAMC,KAC7BA,EAAE9C,MAAMM,KAAKT,OAAO0E,SAJG,ICxL7B,SAASmB,EAAY/C,EAAWzG,GAC9B,MAAM4F,EAAWa,EAAUoB,QAAQ,KAC/BjC,IACF5F,EAAI2D,OAAOiC,SAAWA,GAEpBa,EAAU6C,MAAM,MAAM7C,EAAUjG,MAAM,iCAO5C,SAASiJ,EAAYhD,EAAWiD,GAC9B,IAAI5B,EAhFN,SAAsBrB,EAAWiD,GAC/B,MAAMvG,EAAOsD,EAAUoB,QACrB,cACA,kBACA,UACA,WACA,UAEF,IAAK1E,EACH,OAEF,MAAM2E,EAAMC,EACV,IAAI4B,KAAK,CAAElJ,OAAQgG,EAAUhG,OAAQkD,OAAQ,CAAER,KAAAA,MAKjD,OAHA2E,EAAInE,OAAOuE,KACTzB,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,4BAA4B2C,EAAK5C,SAC3C4C,EAAK5C,OACX,IAAK,UAAW,CACVkG,EAAU6C,MAAM,MAClB7C,EAAUjG,MAAM,+CAClB,MAAMqF,EACJ+D,EAAYnD,EAAWiD,IACvBjD,EAAUjG,MAAM,2BAClBsH,EAAIjC,QAAQuD,KAAKvD,GACjB,MAEF,IAAK,WACL,IAAK,cACL,IAAK,kBAAmB,CACtB,MAAMA,EACJgE,EAA8BpD,EAAWiD,IACzCjD,EAAUjG,MAAM,WAAW2C,EAAK5C,iBAClCuH,EAAIjC,QAAQuD,KAAKvD,GACjB,MAEF,IAAK,SAAU,CACTY,EAAU6C,MAAM,MAClB7C,EAAUjG,MAAM,6CAClB,MAAMsJ,EACJrD,EAAUoB,WAAWkC,IACrBtD,EAAUjG,MAAM,8BAA8BuJ,EAAYlI,KAAK,SAC3DmI,EAAa,IAAIL,KAAK,CAC1BlJ,OAAQgG,EAAUhG,OAClBkD,OAAQ,CAAER,KAAM2G,KAElBE,EAAWrG,OAAOoD,UAChBN,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,uCAClBwJ,EAAWjI,KAAO2H,EAClB,MAAMO,EACJJ,EAA8BpD,EAAWiD,IACzCjD,EAAUjG,MAAM,qCAClBsH,EAAIjC,QAAQuD,KAAKY,EAAYC,GAC7B,OAOJ,OAJKnC,EAAI9C,SAASyB,EAAUjG,MAAM,8BAA8B2C,EAAK5C,SACrEuH,EAAInE,OAAO0E,MACT5B,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,iCAAiC2C,EAAK5C,SACjDuH,EAAI1D,KAmBD8F,CAAazD,EAAWiD,IAAaS,EAAe1D,GAC9D,IAAKqB,EAAK,CACR,MAAM3E,EACJsD,EAAUC,YAAY,eACtBD,EAAUoB,WAAWkC,KAAgBK,GACvC,IAAKjH,EACH,OAEF2E,EAAM,IAAI6B,KAAK,CAAElJ,OAAQgG,EAAUhG,OAAQkD,OAAQ,CAAER,KAAAA,KACjDsD,EAAU6C,MAAM,MAClB7C,EAAUjG,MAAM,4BAA4B2C,EAAK5C,SASrD,MAPoB,YAAhBuH,EAAIuC,SAAyB5D,EAAU6C,MAAM,MAC/C7C,EAAUjG,MAAM,mCAElBsH,EAAI/F,KAAO2H,GAAY,KACvBF,EAAY/C,EAAWqB,GACnBA,EAAIlC,UAA4B,QAAhBkC,EAAI9C,SACtByB,EAAUjG,MAAM,sCACXsH,EAsCF,MAAM6B,aAAa1F,KAKxB,aAAawC,EAAWiD,GACtB,OAAOD,EAAYhD,EAAWiD,IArClC,SAAoBjD,EAAW1E,GAC7B,MAAM4B,EAAS,GAEf,GADAA,EAAOuE,KAAOzB,EAAUoB,QAAQ,MAC3BlE,EAAOuE,KAAM,OAClB,MAAMJ,EAAMC,EAAa,IAAI4B,KAAK,CAAElJ,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAE9D,IADAmE,EAAI/F,KAAOA,GAAQ,OACN,CACX,MAAMuI,EACJT,EAA8BpD,IAC9BA,EAAUjG,MAAM,wDACE,QAAhB8J,EAAItF,SACNyB,EAAUjG,MAAM,iDACE,YAAhB8J,EAAID,SACN5D,EAAUjG,MAAM,qDAClBsH,EAAIjC,QAAQuD,KAAKkB,GACjB,MAAMC,EAAK9D,EAAUoB,QAAQ,MAC7B,IAAI0C,EAEG,MADLD,EAAI3G,OAAOoD,UAAYwD,EAW3B,OARIzC,EAAI9C,QAAQhD,OAAS,GACvByE,EAAUjG,MACR,kEAGJmD,EAAO0E,MACL5B,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,2BAC5CgJ,EAAY/C,EAAWqB,GAChBA,EAAI1D,KASkCoG,CAAW/D,EAAWiD,GAGnExF,aAAY,OAAEzD,EAAM,OAAEkD,IACpB4C,MAAM,CAAE9F,OAAAA,EAAQkD,OAAAA,IAChB/D,OAAOC,eAAeuE,KAAM,UAAW,CAAE7D,MAAO,GAAI8D,UAAU,IAC9DD,KAAKqG,SAAW,IAAItB,mBAAmB,CAAE1I,OAAAA,EAAQkD,OAAQ,KAG3D,cACE,OAAIS,KAAKyB,QAAQ7D,QAAUoC,KAAKT,OAAOR,KAC9BiB,KAAKT,OAAOR,KAAK5C,MAEnB,GAET,eACE,OAAOmK,QAAQtG,KAAKT,OAAOiC,UAE7B,YACE,OAAO8E,QAAQtG,KAAKyB,QAAQ7D,UAAYoC,KAAKT,OAAOR,KAEtD,cACE,GAAIiB,KAAKyB,QAAQ7D,OACf,OAAOoC,KAAKyB,QAOd,OAAO,EAJM,CAACzB,KAAKT,OAAOgH,OAAQvG,KAAKT,OAAOR,KAAMiB,KAAKT,OAAOiH,SAC7DC,QAAQlJ,GAAMA,IACdD,KAAKC,GAAMA,EAAEpB,QACbsB,KAAK,MAIV,UAAUoD,GAGR,SAFOb,KAAKqG,SAASzB,SAAS/D,GAET,SAAjBb,KAAKY,QAAoB,CAC3B,MAAMpE,EAAU,sJAGViD,EAAgBO,KAAKT,OAAOR,KAAMiB,KAAM,eAAgBxD,EAAS,CACrEG,SA6EagB,EA7EQqC,KA8EpB,KACLrC,EAAK4B,OAAOR,KAAK5C,MAAQ,gBAF7B,IAAqBwB,EArEjB,MAAM+I,GAAW1G,KAAKe,OAASF,EAAKI,OAAOtF,IAAIqE,KAAKY,SAC9C5B,EAASgB,KAAKe,MAChBf,KACA0G,GAA4B,YAAjBA,EAAQ/I,KACnB+I,EAAQ9F,aACRR,EACJ,GAAIpB,GAAUgB,KAAKwB,SAAU,CAE3B,MAAM,UAAEF,GAAcX,EAA0B3B,EAAQ6B,IAAS,GACjE,GAAIS,EAAW,CACb,MAAMqF,GAAe3G,KAAKe,MAAQO,EAAYtB,MAAMT,OAAOR,KACrDvC,EAAU,yDACViD,EACJkH,EACA3G,KACA,yBACAxD,SAKJ,IAAK,MAAMiF,KAAWzB,KAAKyB,cAClBA,EAAQmD,SAAS/D,GAM9B0B,MAAMC,GA6BJ,OAAOA,EAAEC,GAAGC,KAAK,CACf1C,KAAKqG,SAAS9D,MAAMC,GA7BJ,MAChB,GAAIxC,KAAKe,OAASf,KAAKiG,QACrB,OAAOzD,EAAEC,GAAGC,KAAK,CACfF,EAAE9C,MAAMM,KAAKT,OAAOR,KAAMyD,EAAEC,GAAGwD,SAC/BzD,EAAE9C,MAAMM,KAAKT,OAAOuE,SACjB9D,KAAKyB,QAAQnE,KAAKC,GAAMA,EAAEgF,MAAMC,KACnCA,EAAE9C,MAAMM,KAAKT,OAAO0E,SAGxB,MAAM2C,EAAa5G,KAAKT,OAAOgH,QAAUvG,KAAKT,OAAOR,KAC/CwH,EAASvG,KAAKT,OAAOgH,OACvB,CAACvG,KAAKT,OAAOgH,OAAOpK,MAAOqG,EAAEC,GAAGjF,OAAOwC,KAAKT,OAAOR,KAAKvB,SACxD,GACEqJ,EAAMrE,EAAElB,UACZkB,EAAEC,GAAGC,KAAK,IACL6D,EACHvG,KAAKT,OAAOR,KAAK5C,MACjBqG,EAAE9C,MAAMM,KAAKT,OAAOiH,WAEtB,CACEM,UACE9G,KACF,QACAzB,QAASyB,OAGb,OAAOwC,EAAEC,GAAGC,KAAK,CAACF,EAAEC,GAAGjF,OAAOoJ,EAAWpJ,QAASqJ,KAIlDE,GACAvE,EAAE9C,MAAMM,KAAKT,OAAOiC,UACpBgB,EAAE9C,MAAMM,KAAKT,OAAOoD,cC3QnB,MAAMqE,gBAAgBnH,KAI3B,aAAawC,GACX,MAAMmB,EAASnB,EAAUoB,QAAQ,KACjC,IAAKD,EACH,OAAO,KAET,MAAMxC,EACJiG,EAAY5E,IACZA,EAAUC,YAAY,WACtBD,EAAUoB,QAAQ,OAAQ,IAAK,MAC/BpB,EAAUjG,MAAM,wBACZ8K,EAAa,CAAClG,GACpB,GAAkB,MAAdA,EAAI7E,MAAe,CACrB,MAAM8H,EACJ5B,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,wCAClB8K,EAAWlC,KAAKf,QACX,GAAkB,MAAdjD,EAAI7E,MAAe,CAC5B,MAAM8H,EACJ5B,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,0CAClB8K,EAAWlC,KAAKf,GAElB,OAAO,IAAI+C,QAAQ,CACjB3K,OAAQgG,EAAUhG,OAClBkD,OAAQ,CAAEiE,OAAAA,GACV0D,WAAAA,IAIJpH,aAAY,OAAEzD,EAAM,OAAEkD,EAAM,WAAE2H,IAC5B/E,MAAM,CAAE9F,OAAAA,EAAQkD,OAAAA,IAChB2H,EAAWtI,OAASoB,KACpBxE,OAAOC,eAAeuE,KAAM,aAAc,CAAE7D,MAAO+K,IAGrD,WACE,OAAOC,EAAWnH,KAAKkH,WAAW,IAAIvJ,KAExC,YACE,OAAOwJ,EAAWnH,KAAKkH,WAAW,IAAI/K,MAExC,eACE,OAAOgL,EAAWnH,KAAKkH,WAAW,IAAIE,SAIxC7E,MAAMC,GACJ,OAAOA,EAAEC,GAAGC,KAAK,CACfF,EAAE9C,MAAMM,KAAKT,OAAOiE,WACjBxD,KAAKkH,WAAW5J,KAAKC,GAAMiF,EAAE9C,MAAMnC,QCxCrC,MAAM8J,iBAAiBxH,KAI5B,aAAawC,GACX,MAAMiF,EAAiBjF,EAAU/F,SAE3BiD,EAAS,GACTmE,EAAMC,EACV,IAAI0D,SAAS,CAAEhL,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAK3C,OAHAmE,EAAI2C,SAAWtB,mBAAmBP,MAAMnC,GACxC9C,EAAOgI,SAAWlF,EAAUoB,QAAQ,YACpCC,EAAI9C,QAAU6E,EAA8BpD,EAAW,iBAClDqB,EAAI9C,SAGJrB,EAAOgI,WACVhI,EAAOiI,SAAWnF,EAAUoB,QAAQ,QAEtClE,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUoB,WAAWgE,GAClBlI,EAAOf,MAGZkF,EAAIgE,QAAUnI,EAAOgI,SAAWP,QAAQxC,MAAMnC,GAAa,KACpDqB,EAAI1D,MAHFqC,EAAU4C,UAAUqC,IATpBjF,EAAU4C,UAAUqC,GAe/B,WACE,MAAO,WAET,eACE,QAAStH,KAAKT,OAAOgI,SAEvB,eACE,QAASvH,KAAKT,OAAOiI,SAEvB,WACE,OAAO,EAASxH,KAAKT,OAAOf,KAAKrC,OAMnC,UAAU0E,SACDb,KAAKqG,SAASzB,SAAS/D,SACvBb,KAAKY,QAAQgE,SAAS/D,GAC7B,MAAM5B,EAAS0B,EAA0BX,KAAKY,QAASC,EAAM,CAC3DC,kBAAkB,IAEpB,GAAI7B,EACF,GAAIe,KAAKY,QAAQY,SAAU,CACzB,MAAMhF,EAAU,iDACViD,EACJO,KAAKT,OAAOf,KACZwB,KACA,uBACAxD,QAEG,GAAKwD,KAAKuH,UAiBV,IAAKvH,KAAK0H,QAAS,CACxB,MAAMlL,EAAU,yEACViD,EACJO,KAAKT,OAAOf,KACZwB,KACA,mBACAxD,EACA,CACEG,QAASgL,EAAsC3H,cAxBnD,GACEA,KAAKpB,SACJ8C,EAAgCzC,EAAOsC,WAAYV,IA8C9D,SAAgC6D,GAC9B,MAAM5B,EAAO4B,EAAI9F,OAAO+F,WAAaD,EAAI9F,OAAOkE,KAC1ClD,EAAQkD,EAAK8E,QAAQlD,GAE3B,OADuB5B,EAAK/F,MAAM6C,EAAQ,GAAGiC,MAAMgG,IAAOA,EAAEN,WAhDpDO,CAAuB9H,MACvB,CACA,MAAMxD,EAAU,0EACViD,EACJO,KAAKT,OAAOf,KACZwB,KACA,oBACAxD,EACA,CACEG,SA8CgC+H,EA9Cc1E,KA+CnD,KACL,MAAM4G,EAAamB,EAAcrD,EAAI9D,SACrC8D,EAAInF,OAAOgI,SAAW,IACjBX,EACHjJ,KAAM,WACNxB,MAAO,YAETyK,EAAWpJ,OAAS,IACpBmK,EAAsCjD,EAAtCiD,OATJ,IAA8CjD,EA1B5CnC,MAAMC,GACJ,OAAOA,EAAEC,GAAGC,KAAK,CACf1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOgI,UACpB/E,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAE9C,MAAMM,KAAKT,OAAOiI,UACpBhF,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,OACvCA,KAAK0H,QAAU1H,KAAK0H,QAAQnF,MAAMC,GAAK,GACvCA,EAAE9C,MAAMM,KAAKT,OAAOoD,cAkC1B,SAASgF,EAAsCjD,GAC7C,MAAO,KACLA,EAAIgD,QAAUV,QAAQxC,MAAM,IAAI0D,UAAU,WCjJvC,MAAMC,kBAAkBtI,KAS7B,aAAawC,GAAW,QAAE+F,EAAO,QAAEC,GAAY,IAC7C,MAAM9I,EAAS,CAAE6I,QAAAA,GACX1E,EAAMC,EACV,IAAIwE,UAAU,CAAE9L,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAE5C,OAAI6I,GAA6B,gBAAlBA,EAAQjM,QACrBoD,EAAO+I,YAAcjG,EAAUoB,QAAQ,KACnClE,EAAO+I,cACT5E,EAAIiB,UAAY,GACTjB,IAGN0E,GAAYC,IACf9I,EAAO6I,QAAU/F,EAAUoB,QAAQ,SAAU,SAAU,YAEzDC,EAAI9C,QACF4E,EAAYnD,IAAcA,EAAUjG,MAAM,uBAC5CmD,EAAOf,KACL6D,EAAUC,YAAY,eAAiBD,EAAUoB,QAAQ,YAC3DlE,EAAOuE,KACLzB,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,qBAC5CsH,EAAIiB,UAAYX,EAAc3B,GAC9B9C,EAAO0E,MACL5B,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,0BAC5CmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,wCACXsH,EAAI1D,MAGb,WACE,MAAO,YAET,WACE,MAAM,KAAExB,GAASwB,KAAKT,OACtB,OAAKf,EAGE,EAASA,EAAKrC,OAFZ,GAIX,cACE,OAAK6D,KAAKT,OAAO6I,QAGVpI,KAAKT,OAAO6I,QAAQjM,MAFlB,GAKX,UAAU0E,GAER,SADOb,KAAKqG,SAASzB,SAAS/D,IACzBb,KAAKxB,MAAQ,CAAC,GAAI,UAAU+J,SAASvI,KAAKoI,SAAU,CACvD,MAAM5L,EAAU,qFACViD,EAAgBO,KAAKT,OAAOuE,KAAM9D,KAAM,gBAAiBxD,GAE7DwD,KAAKY,gBACAZ,KAAKY,QAAQgE,SAAS/D,IAE/B,IAAK,MAAM2H,KAAYxI,KAAK2E,gBACnB6D,EAAS5D,SAAS/D,GAK7B0B,MAAMC,GACJ,MAAM,OAAE5D,GAAWoB,KACbyI,EAAOzI,KAAKY,QACd,CACE4B,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,KAAMpB,OAAAA,IAC7C4D,EAAE9C,MAAMM,KAAKT,OAAOuE,MACpBtB,EAAEC,GAAGC,KAAK1C,KAAK2E,UAAUrH,KAAKoH,GAAQA,EAAInC,MAAMC,MAChDA,EAAE9C,MAAMM,KAAKT,OAAO0E,QAEtB,GACJ,OAAOzB,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBxC,KAAKT,OAAOf,KACRgE,EAAE9C,MAAMM,KAAKT,OAAO6I,SACpB5F,EAAE9C,MAAMM,KAAKT,OAAO6I,QAAS5F,EAAEC,GAAGiG,SAAU,CAAET,KAAMjI,KAAMpB,OAAAA,OAC3D6J,EACHjG,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,KAAMpB,OAAAA,KC3Fb,MAAM+J,kBAAkB9I,KAQ7B,aACEwC,GACA,QAAE+F,EAAO,UAAEQ,GAAY,EAAK,SAAEC,GAAW,GAAU,IAEnD,MAAMvB,EAAiBjF,EAAU/F,SAC3BiD,EAAS,CAAE6I,QAAAA,GACX1E,EAAMC,EACV,IAAIgF,UAAU,CAAEtM,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAa5C,GAXK6I,GAAYQ,IACfrJ,EAAO6I,QAAU/F,EAAUoB,QAAQ,YAEjB,YAAhBC,EAAI0E,SAAyB/F,EAAU6C,MAAM,aAC/C7C,EAAUjG,MAAM,4CAElBmD,EAAOsJ,SAAWxG,EAAUoB,QAAQ,YAChCoF,IAAatJ,EAAOsJ,UAAYxG,EAAU6C,MAAM,cAClD7C,EAAUjG,MAAM,+CAElBmD,EAAOR,KAAOsD,EAAUoB,QAAQ,aAC3BlE,EAAOR,KAcZ,OAVA2E,EAAI9C,QACF6E,EAA8BpD,EAAW,mBACzCA,EAAUjG,MAAM,0BAClBmD,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUoB,QAAQ,QAAS,aAC3BpB,EAAUjG,MAAM,0BAClBmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,wCACXsH,EAAI1D,KAbTqC,EAAU4C,UAAUqC,GAgBxB,WACE,MAAO,YAET,cACE,OAAKtH,KAAKT,OAAO6I,QAGVpI,KAAKT,OAAO6I,QAAQjM,MAFlB,GAIX,eACE,QAAS6D,KAAKT,OAAOsJ,SAEvB,WACE,OAAO,EAAS7I,KAAKT,OAAOf,KAAKrC,OAGnC,UAAU0E,GAIR,aAHOb,KAAKqG,SAASzB,SAAS/D,SACvBb,KAAKY,QAAQgE,SAAS/D,GAErBb,KAAKY,QAAQqF,SACnB,IAAK,WACL,IAAK,SAAU,CACb,MAAMzJ,EAAU,4BAA4BwD,KAAKY,QAAQqF,uBACnDxG,EACJO,KAAKT,OAAOf,KACZwB,KACA,oBACAxD,GAEF,MAEF,QAAS,CACP,MAAM,UAAE8E,GACNX,EAA0BX,KAAKY,QAASC,IAAS,GACnD,GAAIS,EAAW,CACb,MAAMqF,GAAe3G,KAAKY,QAAQG,MAAQO,EAAYtB,KAAKY,SACxDrB,OAAOR,KACJvC,EAAU,mDACViD,EACJkH,EACA3G,KACA,oBACAxD,MAQV+F,MAAMC,GACJ,MAAM,OAAE5D,GAAWoB,KACnB,OAAOwC,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAO6I,SACpB5F,EAAE9C,MAAMM,KAAKT,OAAOsJ,UACpBrG,EAAE9C,MAAMM,KAAKT,OAAOR,MACpByD,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,KAAMpB,OAAAA,IAC7C4D,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,KAAMpB,OAAAA,KCxGb,SAAS,EAASyF,GACvB,OAAOA,EAAWyE,WAAW,KAAOzE,EAAWtH,MAAM,GAAKsH,EAWrD,SAASvB,EAAKT,GAAW,OAAEU,EAAM,aAAEgG,EAAY,SAAE/F,EAAW,SACjE,MAAMgG,EAAQjG,EAAOV,GACrB,IAAK2G,EACH,MAAO,GAETA,EAAMzJ,OAAOoD,UAAYN,EAAUoB,QAAQ,KAC3C,MAAMwF,EAAQ,CAACD,GACf,KAAOA,EAAMzJ,OAAOoD,WAAW,CAC7B,MAAMuG,EAAOnG,EAAOV,GACpB,IAAK6G,EAAM,CACJH,GACH1G,EAAUjG,MAAM,qBAAqB4G,KAEvC,MAIF,GAFAkG,EAAK3J,OAAOoD,UAAYN,EAAUoB,QAAQ,KAC1CwF,EAAMjE,KAAKkE,IACNA,EAAK3J,OAAOoD,UAAW,MAE9B,OAAOsG,EAMF,SAAShC,EAAY5E,GAC1B,OACEA,EAAUC,YAAY,UAAW,YACjCD,EAAUoB,QAAQ,OAAQ,QAAS,WAAY,YAAa,OASzD,SAAS0D,GAAW,KAAExJ,EAAI,MAAExB,IACjC,OAAQwB,GACN,IAAK,UACL,IAAK,UACH,MAAO,CAAEA,KAAM,SAAUxB,MAAAA,GAC3B,IAAK,SACH,MAAO,CAAEwB,KAAM,SAAUxB,MAAOA,EAAMY,MAAM,GAAI,IAGpD,OAAQZ,GACN,IAAK,OACL,IAAK,QACH,MAAO,CAAEwB,KAAM,UAAWxB,MAAiB,SAAVA,GACnC,IAAK,WACL,IAAK,YACH,MAAO,CAAEwB,KAAM,WAAYyJ,SAAUjL,EAAM2M,WAAW,MACxD,IAAK,IACH,MAAO,CAAEnL,KAAM,WAAYxB,MAAO,IACpC,IAAK,IACH,MAAO,CAAEwB,KAAM,cACjB,QACE,MAAO,CAAEA,KAAMxB,IAOd,SAAS4J,EAAe1D,GAoB7B,MAAM,OAAEhG,GAAWgG,EACb8G,EApBN,WACE,MAAM5C,EAASlE,EAAUoB,QAAQ,YAC3B1E,EAAOsD,EAAUoB,QAAQ,QAAS,QACxC,GAAI1E,EAAM,CACR,MAAMyH,EAAUnE,EAAUoB,QAAQ,QAClC,OAAO,IAAI8B,KAAK,CAAElJ,OAAAA,EAAQkD,OAAQ,CAAEgH,OAAAA,EAAQxH,KAAAA,EAAMyH,QAAAA,KAEhDD,GAAQlE,EAAUjG,MAAM,gCAabgN,IAVjB,WACE,MAAM7C,EAASlE,EAAUoB,QAAQ,gBAC3B1E,EAAOsD,EAAUoB,QAAQ,QAAS,UACxC,GAAI1E,EACF,OAAO,IAAIwG,KAAK,CAAElJ,OAAAA,EAAQkD,OAAQ,CAAEgH,OAAAA,EAAQxH,KAAAA,KAE1CwH,GAAQlE,EAAUjG,MAAM,8BAIKiN,GACnC,GAAIF,EAAU,OAAOA,EACrB,MAAMpK,EAAOsD,EAAUoB,QACrB,SACA,UACA,OACA,QACA,aAEF,OAAI1E,EACK,IAAIwG,KAAK,CAAElJ,OAAAA,EAAQkD,OAAQ,CAAER,KAAAA,UADtC,EAQK,SAASiF,EAAc3B,GAC5B,OAAOS,EAAKT,EAAW,CACrBU,OAAQsE,SAAS7C,MACjBxB,SAAU,mBAQP,SAASyC,EAA8BpD,EAAWiD,GACvD,MAAMe,EAAWtB,mBAAmBP,MAAMnC,GACpCqB,EAAM6B,KAAKf,MAAMnC,EAAWiD,GAElC,OADI5B,IAAKC,EAAaD,GAAK2C,SAAWA,GAC/B3C,EAOF,SAAS8B,EAAYnD,EAAWiD,GACrC,MAAMY,EAAMX,KAAKf,MAAMnC,EAAWiD,GAAY,eAC9C,GAAIY,EACF,OAAOA,EAET,MAAMoD,EAAYjH,EAAUoB,QAAQ,QACpC,GAAI6F,EAAW,CACb,MAAM5F,EAAM,IAAI6B,KAAK,CACnBlJ,OAAQgG,EAAUhG,OAClBkD,OAAQ,CAAER,KAAMuK,KAGlB,OADA5F,EAAI/F,KAAO,cACJ+F,GAOJ,SAAS6F,EAAYlH,GAC1B,MAAM+F,EAAU/F,EAAUoB,QAAQ,eAClC,IAAK2E,EAAS,OAKd,OAHEO,UAAUnE,MAAMnC,EAAW,CAAE+F,QAAAA,KAC7BD,UAAU3D,MAAMnC,EAAW,CAAE+F,QAAAA,KAC7B/F,EAAUjG,MAAM,4BAOb,SAASoN,EAAmBC,GACjC,MAAMC,EAAQD,EAAIzL,MAAM,MAExB,GAAI0L,EAAM9L,OAAQ,CAChB,MAAM+L,EAAQD,EAAMA,EAAM9L,OAAS,GAAG+L,MAAM,QAC5C,GAAIA,EACF,OAAOA,EAAM,GAGjB,MAAO,GAeF,SAASC,EAAwB5I,GACtC,MAAO,KACL,GAAIA,EAAIqF,SAASzI,OAAQ,CACvB,MAAMyE,EAAY,IAAI6F,UAAU,mBAC1B2B,EAAUvF,wBAAwBE,MAAMnC,GAC9CwH,EAAQtK,OAAOoD,UAAYN,EAAUoB,QAAQ,KAC7C,MAAMqG,EAAW9I,EAAIqF,SAAS,GACzB,MAAM0D,KAAKD,EAASvK,OAAOf,KAAKhB,UACnCsM,EAASvK,OAAOf,KAAKhB,OAAS,IAAIsM,EAASvK,OAAOf,KAAKhB,UAEzDwD,EAAIqF,SAASxH,QAAQgL,OAChB,CACLlG,EAAa3C,GAAKqF,SAAWtB,mBAAmBP,MAC9C,IAAI0D,UAAU,qBAEhB,MAAM1K,EAASwD,EAAIzB,OAAOR,KAAKvB,OAC/BwD,EAAIqF,SAAS9G,OAAOuE,KAAKtG,OAASA,EAClCwD,EAAIzB,OAAOR,KAAKvB,OAAS,KAAKgM,EAAmBhM,OAShD,SAASuK,EAAcE,GAC5B,GAAIA,EAAK5B,SAASzI,OAChB,OAAOqK,EAAK5B,SAAS9G,OAAOuE,KAE9B,GAAkB,cAAdmE,EAAKtK,OAAyBsK,EAAKG,QACrC,OAAOL,EAAcE,EAAKrH,SAG5B,OADepF,OAAOwO,OAAO/B,EAAK1I,QAAQ0K,MAAK,CAACC,EAAGC,IAAMD,EAAEtK,MAAQuK,EAAEvK,QACvD,GAwBT,SAAS+D,EAAasE,EAAMrJ,GAKjC,GAJKA,IAEHA,EAASqJ,IAENA,EAGH,OAAOA,EA8BT,OA5Bc,IAAImC,MAAMnC,EAAM,CAC5BtM,IAAIqD,EAAQoF,GACV,MAAMjI,EAAQ6C,EAAOoF,GACrB,OAAIlC,MAAMmI,QAAQlO,IAAgB,WAANiI,EAGnBT,EAAaxH,EAAO6C,GAEtB7C,GAETkF,IAAIrC,EAAQoF,EAAGjI,GAGb,GADA6C,EAAOoF,GAAKjI,GACPA,EACH,OAAO,EACF,GAAI+F,MAAMmI,QAAQlO,GAEvB,IAAK,MAAM+M,KAAQ/M,OACU,IAAhB+M,EAAKtK,SACdsK,EAAKtK,OAASA,aAGe,IAAjBzC,EAAMyC,SACtBzC,EAAMyC,OAASA,GAEjB,OAAO,KCtSb,MAAM0L,EAAU,CAGdC,QACE,sGACFC,QAAS,8CACTnG,WAAY,+BACZoG,OAAQ,WACRC,WAAY,cACZC,QAAS,2BACTC,MAAO,wBAGI5E,EAAmB,CAC9B,cACA,WACA,YACA,aACA,aACA,aACA,cACA,cACA,oBACA,gBACA,iBACA,eACA,eACA,MACA,SACA,UAGWL,EAAc,CAAC,aAAc,YAAa,aAE1C8B,EAAuB,CAClC,QACA,YACA,WACA,QACA,cACA,UACA,aACA,OACA,SACA,WACA,UACA,YACA,WACA,UACA,YACA,UACA,WACA,UACA,SACA,SACA,cACA,UACA,gBAGIoD,EAAoB,CACxB,YACA,cACA,WACA,MACA,kBACA,UACA,SACA,UACA,OACA,SACA,QACA,QACA,OACA,QACA,OACA,QACA,WACA,KACA,WACA,SACA,WACA,QACA,OACA,YACA,WACA,QACAC,OAAOrD,EAAsB9B,EAAaK,GAEtC+E,EAAe,CACnB,IACA,IACA,IACA,MACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,IACA,KAGIC,EAAW,CAEf,eACA,WACA,aAgHK,MAAM9C,UAIXpI,YAAYmL,GACVjL,KAAK3D,OA9GT,SAAkBoN,GAChB,MAAMlK,EAAS,GACf,IAAI2L,EAAgB,EAChB1N,EAAS,GACTK,EAAO,EACP+B,EAAQ,EACZ,KAAOsL,EAAgBzB,EAAI7L,QAAQ,CACjC,MAAMuN,EAAW1B,EAAI2B,OAAOF,GAC5B,IAAIjM,GAAU,EAQd,GANI,YAAY8K,KAAKoB,GACnBlM,EAASoM,EAAkB,aAAc,CAAEC,eAAe,IACpC,MAAbH,IACTlM,EAASoM,EAAkB,UAAW,CAAEC,eAAe,MAGzC,IAAZrM,EAAe,CACjB,MAAMsM,EAAgBhM,EAAOiM,MAAMrP,MACnC0B,IAAS0N,EAAc5B,MAAM,QAAU,IAAI/L,OAC3CJ,GAAU+N,EACV3L,GAAS,OACJ,GAAI,iBAAiBmK,KAAKoB,IAK/B,GAJAlM,EAASoM,EAAkB,YACX,IAAZpM,IACFA,EAASoM,EAAkB,aAEb,IAAZpM,EAAe,CACjBA,EAASoM,EAAkB,cAC3B,MAAMI,EAAYlM,EAAO3B,OAAS,EAC5B8B,EAAQH,EAAOkM,GACrB,IAAgB,IAAZxM,EAAe,CACjB,GAAI+L,EAASzC,SAAS7I,EAAMvD,OAAQ,CAClC,MAAMK,EAAU,GAAG,EACjBkD,EAAMvD,wDAER,MAAM,IAAIuP,iBACRlM,EAAYD,EAAQkM,EAAW,KAAMjP,IAE9BqO,EAAkBtC,SAAS7I,EAAMvD,SAC1CuD,EAAM/B,KAAO,gBAIG,MAAbwN,IACTlM,EAASoM,EAAkB,WAG7B,IAAK,MAAMM,KAAeZ,EACxB,GAAItB,EAAIX,WAAW6C,EAAaT,GAAgB,CAC9C3L,EAAOyF,KAAK,CACVrH,KAAM,SACNxB,MAAOwP,EACPnO,OAAAA,EACAK,KAAAA,EACA+B,MAAAA,IAEFpC,EAAS,GACT0N,GAAiBS,EAAY/N,OAC7BqB,EAASiM,EACT,MAQJ,IAHgB,IAAZjM,IACFA,EAASoM,EAAkB,WAEb,IAAZpM,EACF,MAAM,IAAI2M,MAAM,gCAElBV,EAAgBjM,EAChBW,GAAS,EAYX,OARAL,EAAOyF,KAAK,CACVrH,KAAM,MACNxB,MAAO,GACPqB,OAAAA,EACAK,KAAAA,EACA+B,MAAAA,IAGKL,EAOP,SAAS8L,EAAkB1N,GAAM,cAAE2N,GAAkB,IACnD,MAAMO,EAAKvB,EAAQ3M,GACnBkO,EAAGJ,UAAYP,EACf,MAAMjM,EAAS4M,EAAGC,KAAKrC,GACvB,OAAIxK,GACFM,EAAOyF,KAAK,CAAErH,KAAAA,EAAMxB,MAAO8C,EAAO,GAAIzB,OAAAA,EAAQK,KAAAA,EAAM+B,MAAAA,IAC/C0L,IACH9N,EAAS,IAEJqO,EAAGJ,YAEJ,GASMM,CAASd,GACvBjL,KAAK1D,SAAW,EAOlBF,MAAMI,GACJ,MAAM,IAAIkP,iBACRlM,EAAYQ,KAAK3D,OAAQ2D,KAAK1D,SAAU0D,KAAKzD,QAASC,IAO1DwP,UAAUrO,GACR,OACEqC,KAAK3D,OAAOuB,OAASoC,KAAK1D,UAC1B0D,KAAK3D,OAAO2D,KAAK1D,UAAUqB,OAASA,EAOxCuH,MAAM/I,GACJ,OACE6D,KAAKgM,UAAU,WAAahM,KAAK3D,OAAO2D,KAAK1D,UAAUH,QAAUA,EAOrEmG,eAAe2J,GACb,IAAK,MAAMtO,KAAQsO,EAAY,CAC7B,IAAKjM,KAAKgM,UAAUrO,GAAO,SAC3B,MAAM+B,EAAQM,KAAK3D,OAAO2D,KAAK1D,UAE/B,OADA0D,KAAK1D,WACEoD,GAOX+D,WAAWwI,GACT,IAAKjM,KAAKgM,UAAU,UAAW,OAC/B,MAAMtM,EAAQM,KAAK3D,OAAO2D,KAAK1D,UAC/B,IAAK,MAAMH,KAAS8P,EAClB,GAAIvM,EAAMvD,QAAUA,EAEpB,OADA6D,KAAK1D,WACEoD,EAOXwM,kBAAkB/P,GAChB,GAAK6D,KAAKgM,UAAU,eAGhBhM,KAAK3D,OAAO2D,KAAK1D,UAAUH,QAAUA,EAGzC,OAAO6D,KAAKsC,YAAY,cAM1B2C,UAAU3I,GACR0D,KAAK1D,SAAWA,GAIb,MAAMoP,yBAAyBE,MAWpC9L,aAAY,QACVtD,EAAO,YACP4C,EAAW,QACXb,EAAO,KACPV,EAAI,WACJwB,EAAU,MACVC,EAAK,OACLC,IAEA4C,MAAM3F,GAENwD,KAAKxB,KAAO,mBACZwB,KAAKZ,YAAcA,EACnBY,KAAKzB,QAAUA,EACfyB,KAAKnC,KAAOA,EACZmC,KAAKX,WAAaA,EAClBW,KAAKV,MAAQA,EACbU,KAAKT,OAASA,GChVX,MAAM4M,kBAAkB/J,aAI7B,aAAaC,GACX,MAAMlG,EAAQkG,EAAUC,YAAY,UACpC,GAAInG,EACF,OAAO,IAAIgQ,UAAU,CAAE9P,OAAQgG,EAAUhG,OAAQkD,OAAQ,CAAEpD,MAAAA,KAI/D,WACE,MAAO,aAET,YACE,OAAOgG,MAAMhG,MAAMY,MAAM,GAAI,GAI/BwF,MAAMC,GACJ,MAAM,OAAE5D,GAAWoB,KACnB,OAAOwC,EAAEC,GAAGC,KAAK,CACfF,EAAEC,GAAGjF,OAAOwC,KAAKT,OAAOpD,MAAMqB,QAC9BgF,EAAEC,GAAGpH,WACHmH,EAAEC,GAAGC,KAAK,CAAC,IAAKF,EAAEC,GAAGjE,KAAKwB,KAAK7D,MAAO,CAAE8L,KAAMjI,KAAMpB,OAAAA,IAAW,MAC/D,CAAEqJ,KAAMjI,KAAMpB,OAAAA,IAEhB4D,EAAE9C,MAAMM,KAAKT,OAAOoD,cAKnB,MAAMyJ,aAAavM,KAIxB,aAAawC,GAEX,MAAM9C,EAAS,GAEf,GADAA,EAAOR,KAAOsD,EAAUoB,QAAQ,SAC3BlE,EAAOR,KACV,OAEFQ,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,oBAClB,MAAMsH,EAAMC,EAAa,IAAIyI,KAAK,CAAE/P,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAkB9D,OAjBA8C,EAAU9F,QAAUmH,EAAI1D,KACxBT,EAAOuE,KAAOzB,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,iBACxDsH,EAAIsG,OAASlH,EAAKT,EAAW,CAC3BU,OAAQoJ,UAAU3H,MAClBuE,cAAc,EACd/F,SAAU,gBAERX,EAAU2J,UAAU,WACtB3J,EAAUjG,MAAM,gCAElBmD,EAAO0E,MACL5B,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,4BACvCsH,EAAIsG,OAAOpM,QACdyE,EAAUjG,MAAM,oBAElBmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,2BACrCsH,EAAI1D,KAGb,WACE,MAAO,OAET,WACE,OAAO,EAASA,KAAKT,OAAOf,KAAKrC,OAInCoG,MAAMC,GACJ,OAAOA,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOR,MACpByD,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,OACvCwC,EAAE9C,MAAMM,KAAKT,OAAOuE,MACpBtB,EAAEC,GAAGC,KAAK1C,KAAKgK,OAAO1M,KAAK+O,GAAMA,EAAE9J,MAAMC,MACzCA,EAAE9C,MAAMM,KAAKT,OAAO0E,OACpBzB,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,QCvFP,MAAMsM,iBAAiBzM,KAI5B,aAAawC,GACX,MAAMrD,EAASqD,EAAUC,YAAY,cACrC,IAAKtD,EACH,OAEF,MAAMO,EAAS,CAAEP,OAAAA,GAEjB,GADAO,EAAOgJ,SAAWlG,EAAUoB,QAAQ,YAC/BlE,EAAOgJ,SAUZ,OANAhJ,EAAOgN,MACLlK,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,iCAClBmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,2CACX,IAAIkQ,SAAS,CAAEjQ,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAT9C8C,EAAU4C,UAAUjG,EAAOY,OAY/B,WACE,MAAO,WAET,aACE,OAAO,EAASI,KAAKT,OAAOP,OAAO7C,OAErC,eACE,OAAO,EAAS6D,KAAKT,OAAOgN,MAAMpQ,OAIpCoG,MAAMC,GACJ,OAAOA,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE2B,gBAAgBnE,KAAKT,OAAOP,OAAQgB,MACtCwC,EAAE9C,MAAMM,KAAKT,OAAOgJ,UACpB/F,EAAE2B,gBAAgBnE,KAAKT,OAAOgN,MAAOvM,MACrCwC,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,QCxCP,MAAMwM,gBAAgB3M,KAI3B,aAAawC,GAEX,MAAM9C,EAAS,GACTmE,EAAMC,EAAa,IAAI6I,QAAQ,CAAEnQ,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAEjE,GADAA,EAAOR,KAAOsD,EAAUoB,QAAQ,WAC3BlE,EAAOR,KAaZ,OAVA2E,EAAI9C,QACF6E,EAA8BpD,EAAW,iBACzCA,EAAUjG,MAAM,wBAClBmD,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,wBAClBiG,EAAU9F,QAAUmH,EAAI1D,KACxBT,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,sCACXsH,EAAI1D,KAGb,WACE,MAAO,UAET,WACE,OAAO,EAASA,KAAKT,OAAOf,KAAKrC,OAGnC,UAAU0E,SACDb,KAAKY,QAAQgE,SAAS/D,GAI/B0B,MAAMC,GACJ,OAAOA,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOR,MACpByD,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,OACvCwC,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,QC7CP,MAAMyM,yBAAyB5M,KAIpC,aAAawC,EAAWtD,GACtB,MAAMQ,EAAS,CAAER,KAAAA,GACX2E,EAAMC,EACV,IAAI8I,iBAAiB,CAAEpQ,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAmBnD,OAjBAA,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,yBAClBiG,EAAU9F,QAAUmH,EAAI1D,KACxBT,EAAOiE,OACLnB,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,gCAC5CsH,EAAI9C,QACF4E,EAAYnD,IAAcA,EAAUjG,MAAM,gCAC5CmD,EAAOuE,KACLzB,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,4CAClBsH,EAAIiB,UAAYX,EAAc3B,GAC9B9C,EAAO0E,MACL5B,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,yBAC5CmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,uCACXsH,EAAI1D,KAGb,WACE,MAAO,WAET,WACE,OAAO,EAASA,KAAKT,OAAOf,KAAKrC,OAGnC,UAAU0E,SACDb,KAAKqG,SAASzB,SAAS/D,SACvBb,KAAKY,QAAQgE,SAAS/D,GAI/B0B,MAAMC,GACJ,OAAOA,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOR,MACpByD,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,OACvCwC,EAAE9C,MAAMM,KAAKT,OAAOiE,QACpBhB,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAE9C,MAAMM,KAAKT,OAAOuE,SACjB9D,KAAK2E,UAAUrH,KAAKoH,GAAQA,EAAInC,MAAMC,KACzCA,EAAE9C,MAAMM,KAAKT,OAAO0E,OACpBzB,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,QC7CP,MAAM0M,kBAAkB7M,KAM7B,aAAawC,EAAWsK,GAAU,YAAEC,EAAW,eAAEC,IAC/C,MAAM,OAAEtN,EAAM,KAAE5B,GAASgP,EAWzB,IAVApN,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,mBAAmBuB,KACrC0E,EAAU9F,QAAUoQ,EACpBA,EAAWhJ,EAAagJ,GACpBC,GACFpR,OAAOgI,OAAOjE,EAzBpB,SAAqB8C,GACnB,MAAMyK,EAAQzK,EAAUoB,QAAQ,KAChC,OAAKqJ,EAME,CAAEA,MAAAA,EAAOzM,YAFdgC,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,6BAJT,GAsBiBiE,CAAYgC,IAEpC9C,EAAOuE,KAAOzB,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,YAAYuB,KACpEgP,EAAS/K,QAAU,KACN,CAEX,GADArC,EAAO0E,MAAQ5B,EAAUoB,QAAQ,KAC7BlE,EAAO0E,MAIT,OAHA1E,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,2BAA2BuB,KACtCgP,EAAS3M,KAElB,MAAMmF,EAAKJ,mBAAmBP,MAAMnC,GACpC,IAAI0K,EACJ,IAAK,MAAOhK,KAAWiK,KAASH,EAE9B,GADAE,EAAMpJ,EAAaZ,EAAOV,KAAc2K,IACpCD,EACF,MAGCA,GACH1K,EAAUjG,MAAM,kBAElB2Q,EAAI1G,SAAWlB,EACfwH,EAAS/K,QAAQoD,KAAK+H,EAAI/M,OAI9B,cACE,QAASA,KAAKT,OAAOd,QAEvB,WACE,OAAO,EAASuB,KAAKT,OAAOf,KAAKrC,OAEnC,kBACE,OAAK6D,KAAKT,OAAOc,YAGV,EAASL,KAAKT,OAAOc,YAAYlE,OAF/B,KAKX,UAAU0E,GACR,IAAK,MAAMoM,KAAUjN,KAAK4B,QACpBqL,EAAOrI,iBACFqI,EAAOrI,SAAS/D,IAM7B0B,MAAMC,GAcJ,OAAOA,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAO2N,UACpB1K,EAAE9C,MAAMM,KAAKT,OAAOd,SACpB+D,EAAE9C,MAAMM,KAAKT,OAAOR,MACpByD,EAAE9C,MAAMM,KAAKT,OAAOgN,OACpB/J,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,OApBvB,KACbA,KAAKT,OAAOc,YAGVmC,EAAEC,GAAGC,KAAK,CACfF,EAAE9C,MAAMM,KAAKT,OAAOuN,OACpBtK,EAAEC,GAAGjF,OAAOwC,KAAKT,OAAOc,YAAY7C,QACpCgF,EAAEC,GAAGpC,YACHmC,EAAElB,UAAUtB,KAAKT,OAAOc,YAAYlE,MAAO,CAAEoC,QAASyB,UANjD,GAmBPK,GACAmC,EAAE9C,MAAMM,KAAKT,OAAOuE,MACpBtB,EAAEC,GAAGC,KAAK1C,KAAK4B,QAAQtE,KAAK6P,GAAMA,EAAE5K,MAAMC,MAC1CA,EAAE9C,MAAMM,KAAKT,OAAO0E,OACpBzB,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,QCpGP,MAAMoN,iBAAiBvN,KAI5B,aAAawC,GAEX,MAAM9C,EAAS,GAEf,GADAA,EAAOR,KAAOsD,EAAUoB,QAAQ,UAC3BlE,EAAOR,KACV,OAEF,IAAI6B,EAAUmF,EAAe1D,GAC7B,IAAKzB,EAAS,CACZ,MAAM7B,EACJsD,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,sBAClBwE,EAAU,IAAI2E,KAAK,CAAElJ,OAAQgG,EAAUhG,OAAQkD,OAAQ,CAAER,KAAAA,KAEvDsD,EAAU6C,MAAM,MAClB7C,EAAUjG,MAAM,qCAElBwE,EAAQjD,KAAO,aACf4B,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,sBAClBmD,EAAOiE,OACLnB,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,gCAC5CmD,EAAOpD,MACL8K,EAAY5E,IAAcA,EAAUjG,MAAM,uBAC5CmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,oCAClB,MAAMsH,EAAM,IAAI0J,SAAS,CAAE/Q,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAErD,OADAoE,EAAaD,GAAK9C,QAAUA,EACrB8C,EAGT,WACE,MAAO,QAET,WACE,OAAO,EAAS1D,KAAKT,OAAOf,KAAKrC,OAEnC,YACE,OAAOgL,EAAWnH,KAAKT,OAAOpD,OAIhCoG,MAAMC,GACJ,MAAM,OAAE5D,GAAWoB,KACnB,OAAOwC,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOR,MACpByD,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,KAAMpB,OAAAA,IAC7C4D,EAAE9C,MAAMM,KAAKT,OAAOiE,QACpBhB,EAAE9C,MAAMM,KAAKT,OAAOpD,OACpBqG,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,KAAMpB,OAAAA,KC/Db,MAAMyO,qBAAqBxN,KAIhC,aAAawC,GACX,MAAMiF,EAAiBjF,EAAU/F,SAC3BoH,EAAMC,EACV,IAAI0J,aAAa,CAAEhR,OAAQgG,EAAUhG,OAAQkD,OAAQ,OAEjD,OAAEA,GAAWmE,EAUnB,GATAnE,EAAOsJ,SAAWxG,EAAUoB,QAAQ,YAC/BlE,EAAOsJ,WACVtJ,EAAO+N,MAAQjL,EAAUoB,QAAQ,UAEnClE,EAAOR,KAAOQ,EAAOsJ,SACjBxG,EAAUoB,QAAQ,UAAW,WAC7BlE,EAAO+N,MACPjL,EAAUoB,QAAQ,YAClBpB,EAAUoB,QAAQ,WAAY,UAAW,YACxClE,EAAOR,KAEV,YADAsD,EAAU4C,UAAUqC,GAItB,MAAM,KAAE3J,GAAS+F,EACX6J,EAA8B,YAAT5P,EACrB6P,EAAoBD,GAA+B,aAAT5P,EAC1C8P,EAAkB/J,EAAI4J,OAAkB,aAAT3P,EAErC4B,EAAOuE,KACLzB,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,mCAAmCuB,iBACrD,MAAMqL,EACJvD,EAA8BpD,IAC9BA,EAAUjG,MAAM,8BAA8BuB,iBAiChD,OAhCA+F,EAAI9C,QAAU,CAACoI,GACftF,EAAIiB,UAAY,GAEZ6I,IACFxE,EAAMzJ,OAAOoD,UAAYN,EAAUoB,QAAQ,KACvCuF,EAAMzJ,OAAOoD,UACfe,EAAI9C,QAAQoE,KAAKS,EAA8BpD,IACtCkL,GACTlL,EAAUjG,MAAM,mCAAmCuB,kBAIvD4B,EAAO0E,MACL5B,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,sCAAsCuB,iBAEpD0E,EAAU6C,MAAM,OACduI,GACFlO,EAAOmO,SAAWrL,EAAUoB,QAAQ,KACpCC,EAAIiB,UAAUK,QAAQhB,EAAc3B,IACpC9C,EAAOoO,UACLtL,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,8CAElBiG,EAAUjG,MAAM,oDAIpBmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,2BAA2BuB,iBAEtC+F,EAAI1D,KAGb,WACE,OAAOA,KAAKT,OAAOR,KAAK5C,MAE1B,eACE,QAAS6D,KAAKT,OAAOsJ,SAEvB,YACE,QAAS7I,KAAKT,OAAO+N,MAGvB,UAAUzM,GACR,IAAK,MAAMlD,KAAQqC,KAAKY,cACfjD,EAAKiH,SAAS/D,GAEvB,IAAK,MAAM2H,KAAYxI,KAAK2E,gBACnB6D,EAAS5D,SAAS/D,GAK7B0B,MAAMC,GACJ,OAAOA,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOsJ,UACpBrG,EAAE9C,MAAMM,KAAKT,OAAO+N,OACpB9K,EAAE9C,MAAMM,KAAKT,OAAOR,KAAMyD,EAAEC,GAAGwD,SAC/BzD,EAAE9C,MAAMM,KAAKT,OAAOuE,MACpBtB,EAAEC,GAAGC,KAAK1C,KAAKY,QAAQtD,KAAKC,GAAMA,EAAEgF,MAAMC,MAC1CA,EAAE9C,MAAMM,KAAKT,OAAO0E,OACpBzB,EAAE9C,MAAMM,KAAKT,OAAOmO,UACpBlL,EAAEC,GAAGC,KAAK1C,KAAK2E,UAAUrH,KAAKoH,GAAQA,EAAInC,MAAMC,MAChDA,EAAE9C,MAAMM,KAAKT,OAAOoO,WACpBnL,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,KAAMpB,OAAQoB,KAAKpB,UC7G1B,MAAMgP,oBAAoB/N,KAI/B,aAAawC,GACX,MAAMtD,EAAOsD,EAAUoB,QAAQ,eAC/B,IAAK1E,EACH,OAGF,MAAMQ,EAAS,CAAER,KAAAA,GACjBQ,EAAOuE,KACLzB,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,mCAClB,MAAM4Q,EAAOhJ,EAAc3B,GAC3B9C,EAAO0E,MACL5B,EAAUoB,QAAQ,MAAQpB,EAAUjG,MAAM,4BAC5CmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,kCAClB,MAAMsH,EAAM,IAAIkK,YAAY,CAAEvR,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAExD,OADAoE,EAAaD,GAAKiB,UAAYqI,EACvBtJ,EAGT,WACE,MAAO,cAGT,UAAU7C,GACR,IAAK,MAAM2H,KAAYxI,KAAK2E,gBACnB6D,EAAS5D,SAAS/D,GAK7B0B,MAAMC,GACJ,MAAM,OAAE5D,GAAWoB,KACnB,OAAOwC,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOR,KAAMyD,EAAEC,GAAGiG,SAAU,CAAET,KAAMjI,KAAMpB,OAAAA,IACvD4D,EAAE9C,MAAMM,KAAKT,OAAOuE,MACpBtB,EAAEC,GAAGC,KAAK1C,KAAK2E,UAAUrH,KAAKoH,GAAQA,EAAInC,MAAMC,MAChDA,EAAE9C,MAAMM,KAAKT,OAAO0E,OACpBzB,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,KAAMpB,OAAAA,KC3BpB,SAASiP,EAAcxL,GACrB,MAAM+F,EAAU/F,EAAUoB,QAAQ,UAClC,IAAK2E,EAAS,OAKd,OAHEO,UAAUnE,MAAMnC,EAAW,CAAE+F,QAAAA,KAC7BD,UAAU3D,MAAMnC,EAAW,CAAE+F,QAAAA,KAC7B/F,EAAUjG,MAAM,4BAIb,MAAM0R,kBAAkBpB,UAI7B,aAAarK,EAAWtD,GAAM,QAAEN,EAAU,MAAS,IACjD,MAAMc,EAAS,CAAEd,QAAAA,EAASM,KAAAA,GAC1B,OAAO2N,UAAUlI,MACfnC,EACA,IAAIyL,UAAU,CAAEzR,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAC1C,CACEqN,aAAcnO,EACdoO,eAAgB,CACd,CAACO,SAAS5I,OACV,CAACoJ,YAAYpJ,OACb,CAACqJ,GACD,CAACtE,GACD,CAAC8D,aAAa7I,OACd,CAACmE,UAAUnE,OACX,CAAC2D,UAAU3D,UAMnB,WACE,MAAO,YAGT,UAAU3D,GAER,SADOb,KAAKqG,SAASzB,SAAS/D,IAE3Bb,KAAKvB,SACNuB,KAAKqG,SAAS0H,OAAOtJ,GAA6B,YAAjBA,EAAQjG,OACzC,CACA,MAAMhC,EAAU,oTAKViD,EACJO,KAAKT,OAAOf,KACZwB,KACA,kBACAxD,EACA,CACEG,QAASiN,EAAwB5J,QAIvC,MAAMgO,EAAkBhO,KAAKqG,SAASI,QACnChC,GAA6B,gBAAjBA,EAAQjG,OAEvB,IAAK,MAAMsB,KAAekO,EAAiB,CACzC,MAAMxR,EAAU,oRAIViD,EACJK,EAAYP,OAAOf,KACnBwB,KACA,qBACAxD,EACA,CACEG,QAASsR,EAAmBjO,KAAMF,KAMxC,GADiBE,KAAKqG,SAASxE,MAAM4C,GAA6B,WAAjBA,EAAQjG,OAC3C,CACZ,MAAM0P,EAAmBlO,KAAKqG,SAASI,QACpChC,GAA6B,0BAAjBA,EAAQjG,OAEvB,IAAK,MAAM2P,KAASD,EAAkB,CACpC,MAAM1R,EAAU,uEACViD,EACJ0O,EAAM5O,OAAOf,KACbwB,KACA,0BACAxD,GAIJ,MAAM4R,EAAepO,KAAK4B,QAAQ6E,QAC/BwG,GAA2B,gBAAhBA,EAAOtP,OAErB,IAAK,MAAMwQ,KAASC,EAAc,CAChC,MAAM5R,EAAU,kEACViD,EACJ0O,EAAM5O,OAAOR,KACbiB,KACA,0BACAxD,UAKC2F,MAAMyC,SAAS/D,GACjBb,KAAKvB,gBC7HP,UAA0CoC,EAAMwN,GACrD,MAAMC,EA8CN,SAA6BD,GAC3B,MAAME,EAAMC,EAAcH,GAC1B,MAAO,CACLI,QAAS,IAAIC,IACXH,EAAI9H,QAAQkI,GAAsB,WAAfA,EAAGvG,UAAsB9K,KAAKqR,GAAOA,EAAGnQ,QAE7DoQ,WAAY,IAAIF,IACdH,EAAI9H,QAAQkI,GAAsB,WAAfA,EAAGvG,UAAsB9K,KAAKqR,GAAOA,EAAGnQ,SArDjDqQ,CAAoBR,GAC9BS,EAAWjO,EAAKiO,SAASnT,IAAI0S,EAAE7P,OAAS,GACxCuQ,EAASlO,EAAKmO,SAASrT,IAAI0S,EAAE7P,OAAS,GAC5C,IAAK,MAAMyQ,IAAO,IAAIH,KAAaC,GAAS,CAC1C,MAAMG,EAAYV,EAAcS,GAC1BR,EAAUS,EAAUzI,QAAQoB,GAAoB,WAAdA,EAAEO,UACpCwG,EAAaM,EAAUzI,QAAQoB,GAAoB,WAAdA,EAAEO,gBACtC+G,EAAeV,EAASH,EAAQG,QAASQ,EAAKZ,SAC9Cc,EAAeP,EAAYN,EAAQM,WAAYK,EAAKZ,GAC3DI,EAAQW,SAAST,GAAOL,EAAQG,QAAQY,IAAIV,EAAGnQ,QAC/CoQ,EAAWQ,SAAST,GAAOL,EAAQM,WAAWS,IAAIV,EAAGnQ,QASvD,SAAU2Q,EAAeD,EAAWI,EAAWL,EAAKlQ,GAClD,IAAK,MAAMwQ,KAAYL,EAAW,CAChC,MAAM,KAAE1Q,GAAS+Q,EACjB,GAAI/Q,GAAQ8Q,EAAUlO,IAAI5C,GAAO,CAC/B,MACMhC,EAAU,OADsB,WAArB+S,EAASnH,QAAuB,UAAY,gBAChB5J,uDAA0DO,EAAKP,6CACtGiB,EACJ8P,EAAShQ,OAAOf,KAChByQ,EACA,oBACAzS,KAUR,SAASgS,EAAcH,GACrB,OAAOA,EAAEzM,QAAQ6E,QAAO,EAAG9I,KAAAA,KAAoB,cAATA,KDqF7B6R,CAAgC3O,EAAMb,QAKnD,SAASiO,EAAmBwB,EAAcC,GAExC,OADAD,EAAe9L,EAAa8L,GACrB,KACL,MAAME,EAAcnG,EAClBiG,EAAapJ,SAAS9G,OAAOuE,KAAKtG,QAE9BoS,EAAeH,EAAa7N,QAAQhE,OACtC4L,EAAmBzB,EAAc0H,EAAa7N,QAAQ,IAAIpE,QVqD3D,SAA8BqS,GACnC,MAAMF,EAAcnG,EAAmBqG,GACjCC,EAAWH,EAAYpH,SAAS,MAAQ,KAAO,KACrD,OAAOoH,EAAcG,EUvDfC,CAAqBJ,GACnBK,EAAgBpC,YAAYpJ,MAChC,IAAI0D,UAAU,KAAK0H,oBAErBI,EAAc3J,SAAW,IAAItB,mBAAmB,CAC9C1I,OAAQoT,EAAapT,OACrBkD,OAAQ,KAEVoE,EAAaqM,GAAerL,UAAY+K,EAAmB/K,UAE3D,MAAMsL,EV6FH,SAAuBC,EAAOC,GACnC,MAAMvQ,EAAQsQ,EAAMnT,QAAQqT,UAAUC,UAAUF,GAChD,OAAe,IAAXvQ,EACKA,EAEFsQ,EAAMtS,OAASgC,EAAQ,EUlGN0Q,CACpBb,EAAa7N,SACZuL,GAAiB,gBAAXA,EAAExP,OAEX8R,EAAa7N,QAAQ2O,OAAON,EAAgB,EAAG,EAAGD,GAElD,MAAM,MAAE/L,GAAUwL,EAAalQ,OAC1B0E,EAAMzG,OAAO+K,SAAS,QACzBtE,EAAMzG,QAAU,KAAKmS,KAGvB,MAAM,SAAEtJ,GAAaoJ,EACf7P,EAAQyG,EAASuB,QAAQ8H,GACzBc,EAAUnK,EAASkK,OAAO3Q,EAAO,GAClCyG,EAASzI,OAEHyI,EAASzI,SAAWgC,EAC7ByG,EAASzG,EAAQ,GAAGL,OAAOoD,eAAYvC,EAC7BiG,EAASzG,GAAOL,OAAOf,KAAKhB,OAAOiT,SAC7CpK,EAASzG,GAAOL,OAAOf,KAAKhB,OAASgT,EAAQ,GAAGjR,OAAOf,KAAKhB,QAJ5D6I,EAAS9G,OAAOuE,KAAOuC,EAAS9G,OAAO0E,WAAQ7D,GEpK9C,MAAMsQ,cAAchE,UASzB,aAAarK,EAAWtD,GAAM,QAAEN,GAAY,IAC1C,MAAMc,EAAS,CAAEd,QAAAA,EAASM,KAAAA,GAE1B,GADAQ,EAAOgN,MAAQlK,EAAUoB,QAAQ,SAC5BlE,EAAOgN,MAGZ,OAAOG,UAAUlI,MACfnC,EACA,IAAIqO,MAAM,CAAErU,OAAQgG,EAAUhG,OAAQkD,OAAAA,IACtC,CACEsN,eAAgB,CACd,CAACO,SAAS5I,OACV,CAAC+E,GACD,CAACZ,UAAUnE,MAAO,CAAEoE,WAAW,IAC/B,CAACT,UAAU3D,MAAO,CAAE6D,SAAS,OAMrC,WACE,MAAO,mBC3BJ,MAAMsI,cAAc9Q,KAIzB,aAAawC,GAEX,MAAM9C,EAAS,GACTmE,EAAMC,EAAa,IAAIgN,MAAM,CAAEtU,OAAQgG,EAAUhG,OAAQkD,OAAAA,KAe/D,OAdAmE,EAAI2C,SAAWtB,mBAAmBP,MAAMnC,GACxC9C,EAAOwC,SAAWM,EAAUoB,QAAQ,YACpCC,EAAI9C,QACF6E,EAA8BpD,EAAW,oBACzCA,EAAUjG,MAAM,kCAClBmD,EAAOf,KACL6D,EAAUC,YAAY,eACtBD,EAAUjG,MAAM,kCAClBsH,EAAIgE,QAAUV,QAAQxC,MAAMnC,GACxB9C,EAAOwC,UAAY2B,EAAIgE,SACzBrF,EAAUjG,MAAM,2CAClBmD,EAAO+I,YACLjG,EAAUoB,QAAQ,MAClBpB,EAAUjG,MAAM,gDACXsH,EAAI1D,KAGb,WACE,MAAO,QAET,WACE,OAAO,EAASA,KAAKT,OAAOf,KAAKrC,OAEnC,eACE,QAAS6D,KAAKT,OAAOwC,SAGvB,UAAUlB,SACDb,KAAKY,QAAQgE,SAAS/D,GAI/B0B,MAAMC,GACJ,MAAM,OAAE5D,GAAWoB,KACnB,OAAOwC,EAAEC,GAAGpH,WACVmH,EAAEC,GAAGC,KAAK,CACR1C,KAAKqG,SAAS9D,MAAMC,GACpBA,EAAE9C,MAAMM,KAAKT,OAAOwC,UACpBS,EAAEC,GAAG9E,KAAKqC,KAAKY,QAAQ2B,MAAMC,IAC7BA,EAAEwF,WAAWhI,KAAKT,OAAOf,KAAM,CAAEyJ,KAAMjI,KAAMpB,OAAAA,IAC7CoB,KAAK0H,QAAU1H,KAAK0H,QAAQnF,MAAMC,GAAK,GACvCA,EAAE9C,MAAMM,KAAKT,OAAO+I,eAEtB,CAAEL,KAAMjI,KAAMpB,OAAAA,KCzDb,MAAMgS,mBAAmBlE,UAM9B,aAAarK,GAAW,QAAE5D,GAAY,IACpC,MAAMc,EAAS,CAAEd,QAAAA,GAEjB,GADAc,EAAOR,KAAOsD,EAAUoB,QAAQ,cAC3BlE,EAAOR,KAGZ,OAAO2N,UAAUlI,MACfnC,EACA,IAAIuO,WAAW,CAAEvU,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAC3C,CACEqN,aAAcnO,EACdoO,eAAgB,CAAC,CAAC8D,MAAMnM,UAK9B,WACE,MAAO,cCnBJ,MAAMqM,kBAAkBnE,UAM7B,aAAarK,GAAW,QAAE5D,GAAY,IACpC,MAAMc,EAAS,CAAEd,QAAAA,GAEjB,GADAc,EAAOR,KAAOsD,EAAUoB,QAAQ,aAC3BlE,EAAOR,KAGZ,OAAO2N,UAAUlI,MACfnC,EACA,IAAIwO,UAAU,CAAExU,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAC1C,CACEsN,eAAgB,CACd,CAAClE,UAAUnE,MAAO,CAAEoE,WAAW,EAAMC,UAAU,IAC/C,CAACuE,SAAS5I,OACV,CAAC2D,UAAU3D,MAAO,CAAE6D,SAAS,OAMrC,WACE,MAAO,YAGT,UAAUxH,GACR,IACGb,KAAKvB,SACNuB,KAAKqG,SAAS0H,OAAOtJ,GAA6B,YAAjBA,EAAQjG,OACzC,CACA,MAAMhC,EAAU,gTAKViD,EACJO,KAAKT,OAAOf,KACZwB,KACA,kBACAxD,EACA,CACEG,QAASiN,EAAwB5J,cAIhCmC,MAAMyC,SAAS/D,ICpDnB,MAAMiQ,0BAA0BpE,UAIrC,aAAarK,EAAW6K,GAAU,QAAEzO,EAAU,MAAS,IACrD,MAAMc,EAAS,CAAE2N,SAAAA,GAEjB,GADA3N,EAAOR,KAAOsD,EAAUoB,QAAQ,aAC3BlE,EAAOR,KAGZ,OAAO2N,UAAUlI,MACfnC,EACA,IAAIyO,kBAAkB,CAAEzU,OAAQgG,EAAUhG,OAAQkD,OAAAA,IAClD,CACEqN,aAAcnO,EACdoO,eAAgB,CACd,CAACO,SAAS5I,OACV,CAAC2D,UAAU3D,MAAO,CAAE6D,SAAS,OAMrC,WACE,MAAO,sBCRX,SAAS0I,EAAc1O,EAAW1C,GAChC,MAAMtD,EAASgG,EAAUhG,OAEzB,SAASD,EAAMqN,GACbpH,EAAUjG,MAAMqN,GAGlB,SAAShG,KAAWwI,GAClB,OAAO5J,EAAUoB,WAAWwI,GAY9B,SAAS+E,EAAWC,GAClB,MAAMlS,EAAO0E,EAAQ,aACrB,IAAK1E,EAAM,OAKX,OAHE2R,MAAMlM,MAAMnC,EAAWtD,EAAMkS,IAC7BnD,UAAUtJ,MAAMnC,EAAWtD,EAAMkS,IACjC7U,EAAM,gCAeV,SAASf,IACP,GAAIsE,EAAQuR,YACV,IAAK,MAAMC,KAAcxR,EAAQuR,YAAa,CAC5C,MAAMjS,EAASkS,EAAW9O,GAC1B,GAAIpD,EACF,OAAOA,EAKb,OAxCF,WACE,MAAMiO,EAAWzJ,EAAQ,YACzB,GAAKyJ,EACL,OAAI7K,EAAU6C,MAAM,aACX4L,kBAAkBtM,MAAMnC,EAAW6K,GAErCT,iBAAiBjI,MAAMnC,EAAW6K,GAmCvCA,IACA8D,KAvBJ,WACE,MAAMvS,EAAUgF,EAAQ,WACxB,GAAKhF,EACL,OACEmS,WAAWpM,MAAMnC,EAAW,CAAE5D,QAAAA,KAC9BuS,EAAW,CAAEvS,QAAAA,KACboS,UAAUrM,MAAMnC,EAAW,CAAE5D,QAAAA,KAC7BrC,EAAM,qCAiBNqC,IACAmS,WAAWpM,MAAMnC,IACjB+J,KAAK5H,MAAMnC,IACXmK,QAAQhI,MAAMnC,IACdiK,SAAS9H,MAAMnC,IACfwO,UAAUrM,MAAMnC,GAuBpB,MAAM+O,EAnBN,WACE,IAAK/U,EAAOuB,OAAQ,MAAO,GAC3B,MAAMiD,EAAO,GACb,OAAa,CACX,MAAMsE,EAAKJ,mBAAmBP,MAAMnC,GAC9BrB,EAAM3F,IACZ,IAAK2F,EAAK,CACJmE,EAAGvH,QAAQxB,EAAM,6BACrB,MAEFuH,EAAa3C,GAAKqF,SAAWlB,EAC7BtE,EAAKmE,KAAKhE,GAEZ,MAAMqQ,EAAMzO,IAAI4B,MAAMnC,GAItB,OAHI1C,EAAQ2R,UACVzQ,EAAKmE,KAAKqM,GAELxQ,EAEG0Q,GAEZ,OADIlP,EAAU/F,SAAWD,EAAOuB,QAAQxB,EAAM,uBACvCgV,EAWF,SAAS5M,EAAMiF,EAAK9J,EAAU,IACnC,MAAM0C,EAAY,IAAI6F,UAAUuB,GAKhC,YAJkC,IAAvB9J,EAAQN,aAEjBgD,EAAUhG,OAAOmC,KAAOmB,EAAQN,YAE3B0R,EAAc1O,EAAW1C,GCzHlC,SAAS6R,EAAK9M,GACZ,OAAOA,EAGT,MAAM+M,EAAY,CAChB/O,KAAOuG,GAAUA,EAAMxL,KAAK,IAC5BD,OAAQgU,EACRhT,KAAMgT,EACNlQ,UAAWkQ,EACX7T,KAAM6T,EACNvL,QAASuL,EACT9I,SAAU8I,EACVnR,YAAamR,EACbnW,WAAYmW,EACZ3M,kBAAmB2M,EACnB1M,2BAA4B0M,GAGvB,MAAME,OACX5R,YAAY2C,GACVzC,KAAKyC,GAAKjH,OAAOgI,OAAO,GAAIiO,EAAWhP,GAUzCnB,UAAUqQ,GAAK,UAAE7K,EAAS,QAAEvI,IAI1B,OAHKuI,IACHA,EAAY6K,EAAI7I,WAAW,KAAO6I,EAAI5U,MAAM,GAAK4U,GAE5C3R,KAAKyC,GAAGnB,UAAUqQ,EAAK7K,EAAWvI,GAS3CmB,MAAMnC,EAAGqU,EAAUJ,KAASxE,GAC1B,IAAKzP,EACH,MAAO,GAET,MAAMpB,EAAQyV,EAAQrU,EAAEpB,SAAU6Q,GAClC,OAAOhN,KAAKyC,GAAGC,KAAK,CAAC1C,KAAKyC,GAAGjF,OAAOD,EAAEC,QAASrB,IAGjDgI,gBAAgB5G,EAAGgB,GACjB,OAAOyB,KAAKN,MAAMnC,EAAGyC,KAAKsB,UAAUuQ,KAAK7R,MAAO,CAAEzB,QAAAA,IAGpDyJ,WAAWzK,EAAGmH,GACZ,OAAO1E,KAAKN,MAAMnC,EAAGyC,KAAKyC,GAAGjE,KAAMkG,GAGrCL,WAAWyN,EAAIvT,GACb,OAAOyB,KAAKyC,GAAGC,KAAK,CAClB1C,KAAKmE,gBAAgB2N,EAAGvS,OAAOpD,MAAOoC,GACtCyB,KAAKN,MAAMoS,EAAGvS,OAAOoD,cAKpB,SAASJ,EAAMwP,GAAON,UAAWhP,EAAKgP,GAAc,IACzDhP,EAAKjH,OAAOgI,OAAO,GAAIiO,EAAWhP,GAElC,MAAMD,EAAI,IAAIkP,OAAOjP,GAErB,OAAOA,EAAGC,KAAKqP,EAAIzU,KAAK0U,GAAOA,EAAGzP,MAAMC,MCtE1C,SAASyP,EAAYC,EAAKjR,GACxB,MAAM3D,EAAM,IAAI6F,IACVoF,EAAW2J,EAAIzL,QAAQzF,GAAqB,aAAbA,EAAIrD,OACzC,IAAK,MAAMwU,KAAW5J,EAAU,CAC9B,MAAMgE,EAAQtL,EAAOtF,IAAIwW,EAAQ5J,UACjC,IAAKgE,EACH,SAEF,MAAM2D,EAAQ5S,EAAI3B,IAAIwW,EAAQnT,QAC1BkR,EACFA,EAAMlL,KAAKuH,GAEXjP,EAAI+D,IAAI8Q,EAAQnT,OAAQ,CAACuN,IAG7B,OAAOjP,EAoDT,SAAU8U,EAAiBL,GACzB,MAAMlR,EA/CR,SAA0BqR,GACxB,MAAMjR,EAAS,IAAIkC,IACbkP,EAAa,IAAI3D,IACjBI,EAAW,IAAI3L,IACrB,IAAK,MAAMnC,KAAOkR,EAChB,GAAIlR,EAAIvC,QAAR,CACE,MAAMyR,EAAQpB,EAASnT,IAAIqF,EAAIxC,MAC3B0R,EACFA,EAAMlL,KAAKhE,GAEX8N,EAASzN,IAAIL,EAAIxC,KAAM,CAACwC,SAIvBA,EAAIxC,OAGJyC,EAAOG,IAAIJ,EAAIxC,MAGlB6T,EAAWhD,IAAIrO,GAFfC,EAAOI,IAAIL,EAAIxC,KAAMwC,IAKzB,MAAO,CACLkR,IAAAA,EACAjR,OAAAA,EACA6N,SAAAA,EACAuD,WAAAA,EACArD,SAAUiD,EAAYC,EAAKjR,GAC3BE,MAAO,CACLD,0BAA2B,IAAIoR,QAC/B5Q,gCAAiC,IAAI4Q,UAgB5BC,CAAiBR,GAC9B,IAAK,MAAM/Q,KAAOH,EAAKqR,IACjBlR,EAAI4D,iBACC5D,EAAI4D,SAAS/D,UAd1B,WAA+B,OAAEI,EAAM,WAAEoR,IACvC,IAAK,MAAMG,KAAOH,EAAY,CAC5B,MAAM,KAAE7T,GAASgU,EACXhW,EAAU,aAAagC,eAC3ByC,EAAOtF,IAAI6C,GAAMb,+BAEb,EAAM6U,EAAIjT,OAAOf,KAAMgU,EAAK,eAAgBhW,IAW7CiW,CAAqB5R,GAevB,SAAS+D,EAASmN,GACvB,MAAO,IAAIK,GAZIlC,EAYqB6B,EAXhC7B,EAAMwC,KACDxC,EAAMwC,OAER,GAAG5H,UAAUoF,MAJtB,IAAiBA,E","sources":["webpack://WebIDL2/webpack/universalModuleDefinition","webpack://WebIDL2/webpack/bootstrap","webpack://WebIDL2/webpack/runtime/define property getters","webpack://WebIDL2/webpack/runtime/hasOwnProperty shorthand","webpack://WebIDL2/webpack/runtime/make namespace object","webpack://WebIDL2/./lib/error.js","webpack://WebIDL2/./lib/productions/base.js","webpack://WebIDL2/./lib/validators/helpers.js","webpack://WebIDL2/./lib/productions/array-base.js","webpack://WebIDL2/./lib/productions/token.js","webpack://WebIDL2/./lib/productions/extended-attributes.js","webpack://WebIDL2/./lib/productions/type.js","webpack://WebIDL2/./lib/productions/default.js","webpack://WebIDL2/./lib/productions/argument.js","webpack://WebIDL2/./lib/productions/operation.js","webpack://WebIDL2/./lib/productions/attribute.js","webpack://WebIDL2/./lib/productions/helpers.js","webpack://WebIDL2/./lib/tokeniser.js","webpack://WebIDL2/./lib/productions/enum.js","webpack://WebIDL2/./lib/productions/includes.js","webpack://WebIDL2/./lib/productions/typedef.js","webpack://WebIDL2/./lib/productions/callback.js","webpack://WebIDL2/./lib/productions/container.js","webpack://WebIDL2/./lib/productions/constant.js","webpack://WebIDL2/./lib/productions/iterable.js","webpack://WebIDL2/./lib/productions/constructor.js","webpack://WebIDL2/./lib/productions/interface.js","webpack://WebIDL2/./lib/validators/interface.js","webpack://WebIDL2/./lib/productions/mixin.js","webpack://WebIDL2/./lib/productions/field.js","webpack://WebIDL2/./lib/productions/dictionary.js","webpack://WebIDL2/./lib/productions/namespace.js","webpack://WebIDL2/./lib/productions/callback-interface.js","webpack://WebIDL2/./lib/webidl2.js","webpack://WebIDL2/./lib/writer.js","webpack://WebIDL2/./lib/validator.js"],"sourcesContent":["(function webpackUniversalModuleDefinition(root, factory) {\n\tif(typeof exports === 'object' && typeof module === 'object')\n\t\tmodule.exports = factory();\n\telse if(typeof define === 'function' && define.amd)\n\t\tdefine([], factory);\n\telse if(typeof exports === 'object')\n\t\texports[\"WebIDL2\"] = factory();\n\telse\n\t\troot[\"WebIDL2\"] = factory();\n})(globalThis, () => {\nreturn ","// The require scope\nvar __webpack_require__ = {};\n\n","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","/**\n * @param {string} text\n */\nfunction lastLine(text) {\n  const splitted = text.split(\"\\n\");\n  return splitted[splitted.length - 1];\n}\n\nfunction appendIfExist(base, target) {\n  let result = base;\n  if (target) {\n    result += ` ${target}`;\n  }\n  return result;\n}\n\nfunction contextAsText(node) {\n  const hierarchy = [node];\n  while (node && node.parent) {\n    const { parent } = node;\n    hierarchy.unshift(parent);\n    node = parent;\n  }\n  return hierarchy.map((n) => appendIfExist(n.type, n.name)).join(\" -> \");\n}\n\n/**\n * @typedef {object} WebIDL2ErrorOptions\n * @property {\"error\" | \"warning\"} [level]\n * @property {Function} [autofix]\n * @property {string} [ruleName]\n *\n * @typedef {ReturnType<typeof error>} WebIDLErrorData\n *\n * @param {string} message error message\n * @param {*} position\n * @param {*} current\n * @param {*} message\n * @param {\"Syntax\" | \"Validation\"} kind error type\n * @param {WebIDL2ErrorOptions=} options\n */\nfunction error(\n  source,\n  position,\n  current,\n  message,\n  kind,\n  { level = \"error\", autofix, ruleName } = {}\n) {\n  /**\n   * @param {number} count\n   */\n  function sliceTokens(count) {\n    return count > 0\n      ? source.slice(position, position + count)\n      : source.slice(Math.max(position + count, 0), position);\n  }\n\n  /**\n   * @param {import(\"./tokeniser.js\").Token[]} inputs\n   * @param {object} [options]\n   * @param {boolean} [options.precedes]\n   * @returns\n   */\n  function tokensToText(inputs, { precedes } = {}) {\n    const text = inputs.map((t) => t.trivia + t.value).join(\"\");\n    const nextToken = source[position];\n    if (nextToken.type === \"eof\") {\n      return text;\n    }\n    if (precedes) {\n      return text + nextToken.trivia;\n    }\n    return text.slice(nextToken.trivia.length);\n  }\n\n  const maxTokens = 5; // arbitrary but works well enough\n  const line =\n    source[position].type !== \"eof\"\n      ? source[position].line\n      : source.length > 1\n      ? source[position - 1].line\n      : 1;\n\n  const precedingLastLine = lastLine(\n    tokensToText(sliceTokens(-maxTokens), { precedes: true })\n  );\n\n  const subsequentTokens = sliceTokens(maxTokens);\n  const subsequentText = tokensToText(subsequentTokens);\n  const subsequentFirstLine = subsequentText.split(\"\\n\")[0];\n\n  const spaced = \" \".repeat(precedingLastLine.length) + \"^\";\n  const sourceContext = precedingLastLine + subsequentFirstLine + \"\\n\" + spaced;\n\n  const contextType = kind === \"Syntax\" ? \"since\" : \"inside\";\n  const inSourceName = source.name ? ` in ${source.name}` : \"\";\n  const grammaticalContext =\n    current && current.name\n      ? `, ${contextType} \\`${current.partial ? \"partial \" : \"\"}${contextAsText(\n          current\n        )}\\``\n      : \"\";\n  const context = `${kind} error at line ${line}${inSourceName}${grammaticalContext}:\\n${sourceContext}`;\n  return {\n    message: `${context} ${message}`,\n    bareMessage: message,\n    context,\n    line,\n    sourceName: source.name,\n    level,\n    ruleName,\n    autofix,\n    input: subsequentText,\n    tokens: subsequentTokens,\n  };\n}\n\n/**\n * @param {string} message error message\n */\nexport function syntaxError(source, position, current, message) {\n  return error(source, position, current, message, \"Syntax\");\n}\n\n/**\n * @param {string} message error message\n * @param {WebIDL2ErrorOptions} [options]\n */\nexport function validationError(\n  token,\n  current,\n  ruleName,\n  message,\n  options = {}\n) {\n  options.ruleName = ruleName;\n  return error(\n    current.source,\n    token.index,\n    current,\n    message,\n    \"Validation\",\n    options\n  );\n}\n","export class Base {\n  /**\n   * @param {object} initializer\n   * @param {Base[\"source\"]} initializer.source\n   * @param {Base[\"tokens\"]} initializer.tokens\n   */\n  constructor({ source, tokens }) {\n    Object.defineProperties(this, {\n      source: { value: source },\n      tokens: { value: tokens, writable: true },\n      parent: { value: null, writable: true },\n      this: { value: this }, // useful when escaping from proxy\n    });\n  }\n\n  toJSON() {\n    const json = { type: undefined, name: undefined, inheritance: undefined };\n    let proto = this;\n    while (proto !== Object.prototype) {\n      const descMap = Object.getOwnPropertyDescriptors(proto);\n      for (const [key, value] of Object.entries(descMap)) {\n        if (value.enumerable || value.get) {\n          // @ts-ignore - allow indexing here\n          json[key] = this[key];\n        }\n      }\n      proto = Object.getPrototypeOf(proto);\n    }\n    return json;\n  }\n}\n","/**\n * @typedef {import(\"../productions/dictionary.js\").Dictionary} Dictionary\n *\n * @param {*} idlType\n * @param {import(\"../validator.js\").Definitions} defs\n * @param {object} [options]\n * @param {boolean} [options.useNullableInner] use when the input idlType is nullable and you want to use its inner type\n * @return {{ reference: *, dictionary: Dictionary }} the type reference that ultimately includes dictionary.\n */\nexport function idlTypeIncludesDictionary(\n  idlType,\n  defs,\n  { useNullableInner } = {}\n) {\n  if (!idlType.union) {\n    const def = defs.unique.get(idlType.idlType);\n    if (!def) {\n      return;\n    }\n    if (def.type === \"typedef\") {\n      const { typedefIncludesDictionary } = defs.cache;\n      if (typedefIncludesDictionary.has(def)) {\n        // Note that this also halts when it met indeterminate state\n        // to prevent infinite recursion\n        return typedefIncludesDictionary.get(def);\n      }\n      defs.cache.typedefIncludesDictionary.set(def, undefined); // indeterminate state\n      const result = idlTypeIncludesDictionary(def.idlType, defs);\n      defs.cache.typedefIncludesDictionary.set(def, result);\n      if (result) {\n        return {\n          reference: idlType,\n          dictionary: result.dictionary,\n        };\n      }\n    }\n    if (def.type === \"dictionary\" && (useNullableInner || !idlType.nullable)) {\n      return {\n        reference: idlType,\n        dictionary: def,\n      };\n    }\n  }\n  for (const subtype of idlType.subtype) {\n    const result = idlTypeIncludesDictionary(subtype, defs);\n    if (result) {\n      if (subtype.union) {\n        return result;\n      }\n      return {\n        reference: subtype,\n        dictionary: result.dictionary,\n      };\n    }\n  }\n}\n\n/**\n * @param {*} dict dictionary type\n * @param {import(\"../validator.js\").Definitions} defs\n * @return {boolean}\n */\nexport function dictionaryIncludesRequiredField(dict, defs) {\n  if (defs.cache.dictionaryIncludesRequiredField.has(dict)) {\n    return defs.cache.dictionaryIncludesRequiredField.get(dict);\n  }\n  // Set cached result to indeterminate to short-circuit circular definitions.\n  // The final result will be updated to true or false.\n  defs.cache.dictionaryIncludesRequiredField.set(dict, undefined);\n  let result = dict.members.some((field) => field.required);\n  if (!result && dict.inheritance) {\n    const superdict = defs.unique.get(dict.inheritance);\n    if (!superdict) {\n      // Assume required members in the supertype if it is unknown.\n      result = true;\n    } else if (dictionaryIncludesRequiredField(superdict, defs)) {\n      result = true;\n    }\n  }\n  defs.cache.dictionaryIncludesRequiredField.set(dict, result);\n  return result;\n}\n","export class ArrayBase extends Array {\n  constructor({ source, tokens }) {\n    super();\n    Object.defineProperties(this, {\n      source: { value: source },\n      tokens: { value: tokens },\n      parent: { value: null, writable: true },\n    });\n  }\n}\n","import { Base } from \"./base.js\";\nimport { unescape } from \"./helpers.js\";\n\nexport class WrappedToken extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {string} type\n   */\n  static parser(tokeniser, type) {\n    return () => {\n      const value = tokeniser.consumeKind(type);\n      if (value) {\n        return new WrappedToken({\n          source: tokeniser.source,\n          tokens: { value },\n        });\n      }\n    };\n  }\n\n  get value() {\n    return unescape(this.tokens.value.value);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.wrap([\n      w.token(this.tokens.value),\n      w.token(this.tokens.separator),\n    ]);\n  }\n}\n\nexport class Eof extends WrappedToken {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const value = tokeniser.consumeKind(\"eof\");\n    if (value) {\n      return new Eof({ source: tokeniser.source, tokens: { value } });\n    }\n  }\n\n  get type() {\n    return \"eof\";\n  }\n}\n","import { Base } from \"./base.js\";\nimport { ArrayBase } from \"./array-base.js\";\nimport { WrappedToken } from \"./token.js\";\nimport { list, argument_list, autoParenter, unescape } from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {string} tokenName\n */\nfunction tokens(tokeniser, tokenName) {\n  return list(tokeniser, {\n    parser: WrappedToken.parser(tokeniser, tokenName),\n    listName: tokenName + \" list\",\n  });\n}\n\nconst extAttrValueSyntax = [\"identifier\", \"decimal\", \"integer\", \"string\"];\n\nconst shouldBeLegacyPrefixed = [\n  \"NoInterfaceObject\",\n  \"LenientSetter\",\n  \"LenientThis\",\n  \"TreatNonObjectAsNull\",\n  \"Unforgeable\",\n];\n\nconst renamedLegacies = new Map([\n  .../** @type {[string, string][]} */ (\n    shouldBeLegacyPrefixed.map((name) => [name, `Legacy${name}`])\n  ),\n  [\"NamedConstructor\", \"LegacyFactoryFunction\"],\n  [\"OverrideBuiltins\", \"LegacyOverrideBuiltIns\"],\n  [\"TreatNullAs\", \"LegacyNullToEmptyString\"],\n]);\n\n/**\n * This will allow a set of extended attribute values to be parsed.\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nfunction extAttrListItems(tokeniser) {\n  for (const syntax of extAttrValueSyntax) {\n    const toks = tokens(tokeniser, syntax);\n    if (toks.length) {\n      return toks;\n    }\n  }\n  tokeniser.error(\n    `Expected identifiers, strings, decimals, or integers but none found`\n  );\n}\n\nexport class ExtendedAttributeParameters extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = { assign: tokeniser.consume(\"=\") };\n    const ret = autoParenter(\n      new ExtendedAttributeParameters({ source: tokeniser.source, tokens })\n    );\n    ret.list = [];\n    if (tokens.assign) {\n      tokens.asterisk = tokeniser.consume(\"*\");\n      if (tokens.asterisk) {\n        return ret.this;\n      }\n      tokens.secondaryName = tokeniser.consumeKind(...extAttrValueSyntax);\n    }\n    tokens.open = tokeniser.consume(\"(\");\n    if (tokens.open) {\n      ret.list = ret.rhsIsList\n        ? // [Exposed=(Window,Worker)]\n          extAttrListItems(tokeniser)\n        : // [LegacyFactoryFunction=Audio(DOMString src)] or [Constructor(DOMString str)]\n          argument_list(tokeniser);\n      tokens.close =\n        tokeniser.consume(\")\") ||\n        tokeniser.error(\"Unexpected token in extended attribute argument list\");\n    } else if (tokens.assign && !tokens.secondaryName) {\n      tokeniser.error(\"No right hand side to extended attribute assignment\");\n    }\n    return ret.this;\n  }\n\n  get rhsIsList() {\n    return (\n      this.tokens.assign && !this.tokens.asterisk && !this.tokens.secondaryName\n    );\n  }\n\n  get rhsType() {\n    if (this.rhsIsList) {\n      return this.list[0].tokens.value.type + \"-list\";\n    }\n    if (this.tokens.asterisk) {\n      return \"*\";\n    }\n    if (this.tokens.secondaryName) {\n      return this.tokens.secondaryName.type;\n    }\n    return null;\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { rhsType } = this;\n    return w.ts.wrap([\n      w.token(this.tokens.assign),\n      w.token(this.tokens.asterisk),\n      w.reference_token(this.tokens.secondaryName, this.parent),\n      w.token(this.tokens.open),\n      ...this.list.map((p) => {\n        return rhsType === \"identifier-list\"\n          ? w.identifier(p, this.parent)\n          : p.write(w);\n      }),\n      w.token(this.tokens.close),\n    ]);\n  }\n}\n\nexport class SimpleExtendedAttribute extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const name = tokeniser.consumeKind(\"identifier\");\n    if (name) {\n      return new SimpleExtendedAttribute({\n        source: tokeniser.source,\n        tokens: { name },\n        params: ExtendedAttributeParameters.parse(tokeniser),\n      });\n    }\n  }\n\n  constructor({ source, tokens, params }) {\n    super({ source, tokens });\n    params.parent = this;\n    Object.defineProperty(this, \"params\", { value: params });\n  }\n\n  get type() {\n    return \"extended-attribute\";\n  }\n  get name() {\n    return this.tokens.name.value;\n  }\n  get rhs() {\n    const { rhsType: type, tokens, list } = this.params;\n    if (!type) {\n      return null;\n    }\n    const value = this.params.rhsIsList\n      ? list\n      : this.params.tokens.secondaryName\n      ? unescape(tokens.secondaryName.value)\n      : null;\n    return { type, value };\n  }\n  get arguments() {\n    const { rhsIsList, list } = this.params;\n    if (!list || rhsIsList) {\n      return [];\n    }\n    return list;\n  }\n\n  *validate(defs) {\n    const { name } = this;\n    if (name === \"LegacyNoInterfaceObject\") {\n      const message = `\\`[LegacyNoInterfaceObject]\\` extended attribute is an \\\nundesirable feature that may be removed from Web IDL in the future. Refer to the \\\n[relevant upstream PR](https://github.com/whatwg/webidl/pull/609) for more \\\ninformation.`;\n      yield validationError(\n        this.tokens.name,\n        this,\n        \"no-nointerfaceobject\",\n        message,\n        { level: \"warning\" }\n      );\n    } else if (renamedLegacies.has(name)) {\n      const message = `\\`[${name}]\\` extended attribute is a legacy feature \\\nthat is now renamed to \\`[${renamedLegacies.get(name)}]\\`. Refer to the \\\n[relevant upstream PR](https://github.com/whatwg/webidl/pull/870) for more \\\ninformation.`;\n      yield validationError(this.tokens.name, this, \"renamed-legacy\", message, {\n        level: \"warning\",\n        autofix: renameLegacyExtendedAttribute(this),\n      });\n    }\n    for (const arg of this.arguments) {\n      yield* arg.validate(defs);\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.wrap([\n      w.ts.trivia(this.tokens.name.trivia),\n      w.ts.extendedAttribute(\n        w.ts.wrap([\n          w.ts.extendedAttributeReference(this.name),\n          this.params.write(w),\n        ])\n      ),\n      w.token(this.tokens.separator),\n    ]);\n  }\n}\n\n/**\n * @param {SimpleExtendedAttribute} extAttr\n */\nfunction renameLegacyExtendedAttribute(extAttr) {\n  return () => {\n    const { name } = extAttr;\n    extAttr.tokens.name.value = renamedLegacies.get(name);\n    if (name === \"TreatNullAs\") {\n      extAttr.params.tokens = {};\n    }\n  };\n}\n\n// Note: we parse something simpler than the official syntax. It's all that ever\n// seems to be used\nexport class ExtendedAttributes extends ArrayBase {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const tokens = {};\n    tokens.open = tokeniser.consume(\"[\");\n    const ret = new ExtendedAttributes({ source: tokeniser.source, tokens });\n    if (!tokens.open) return ret;\n    ret.push(\n      ...list(tokeniser, {\n        parser: SimpleExtendedAttribute.parse,\n        listName: \"extended attribute\",\n      })\n    );\n    tokens.close =\n      tokeniser.consume(\"]\") ||\n      tokeniser.error(\n        \"Expected a closing token for the extended attribute list\"\n      );\n    if (!ret.length) {\n      tokeniser.unconsume(tokens.close.index);\n      tokeniser.error(\"An extended attribute list must not be empty\");\n    }\n    if (tokeniser.probe(\"[\")) {\n      tokeniser.error(\n        \"Illegal double extended attribute lists, consider merging them\"\n      );\n    }\n    return ret;\n  }\n\n  *validate(defs) {\n    for (const extAttr of this) {\n      yield* extAttr.validate(defs);\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    if (!this.length) return \"\";\n    return w.ts.wrap([\n      w.token(this.tokens.open),\n      ...this.map((ea) => ea.write(w)),\n      w.token(this.tokens.close),\n    ]);\n  }\n}\n","import { Base } from \"./base.js\";\nimport {\n  unescape,\n  type_with_extended_attributes,\n  return_type,\n  primitive_type,\n  autoParenter,\n} from \"./helpers.js\";\nimport { stringTypes, typeNameKeywords } from \"../tokeniser.js\";\nimport { validationError } from \"../error.js\";\nimport { idlTypeIncludesDictionary } from \"../validators/helpers.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nfunction generic_type(tokeniser, typeName) {\n  const base = tokeniser.consume(\n    \"FrozenArray\",\n    \"ObservableArray\",\n    \"Promise\",\n    \"sequence\",\n    \"record\"\n  );\n  if (!base) {\n    return;\n  }\n  const ret = autoParenter(\n    new Type({ source: tokeniser.source, tokens: { base } })\n  );\n  ret.tokens.open =\n    tokeniser.consume(\"<\") ||\n    tokeniser.error(`No opening bracket after ${base.value}`);\n  switch (base.value) {\n    case \"Promise\": {\n      if (tokeniser.probe(\"[\"))\n        tokeniser.error(\"Promise type cannot have extended attribute\");\n      const subtype =\n        return_type(tokeniser, typeName) ||\n        tokeniser.error(\"Missing Promise subtype\");\n      ret.subtype.push(subtype);\n      break;\n    }\n    case \"sequence\":\n    case \"FrozenArray\":\n    case \"ObservableArray\": {\n      const subtype =\n        type_with_extended_attributes(tokeniser, typeName) ||\n        tokeniser.error(`Missing ${base.value} subtype`);\n      ret.subtype.push(subtype);\n      break;\n    }\n    case \"record\": {\n      if (tokeniser.probe(\"[\"))\n        tokeniser.error(\"Record key cannot have extended attribute\");\n      const keyType =\n        tokeniser.consume(...stringTypes) ||\n        tokeniser.error(`Record key must be one of: ${stringTypes.join(\", \")}`);\n      const keyIdlType = new Type({\n        source: tokeniser.source,\n        tokens: { base: keyType },\n      });\n      keyIdlType.tokens.separator =\n        tokeniser.consume(\",\") ||\n        tokeniser.error(\"Missing comma after record key type\");\n      keyIdlType.type = typeName;\n      const valueType =\n        type_with_extended_attributes(tokeniser, typeName) ||\n        tokeniser.error(\"Error parsing generic type record\");\n      ret.subtype.push(keyIdlType, valueType);\n      break;\n    }\n  }\n  if (!ret.idlType) tokeniser.error(`Error parsing generic type ${base.value}`);\n  ret.tokens.close =\n    tokeniser.consume(\">\") ||\n    tokeniser.error(`Missing closing bracket after ${base.value}`);\n  return ret.this;\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nfunction type_suffix(tokeniser, obj) {\n  const nullable = tokeniser.consume(\"?\");\n  if (nullable) {\n    obj.tokens.nullable = nullable;\n  }\n  if (tokeniser.probe(\"?\")) tokeniser.error(\"Can't nullable more than once\");\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {string} typeName\n */\nfunction single_type(tokeniser, typeName) {\n  let ret = generic_type(tokeniser, typeName) || primitive_type(tokeniser);\n  if (!ret) {\n    const base =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.consume(...stringTypes, ...typeNameKeywords);\n    if (!base) {\n      return;\n    }\n    ret = new Type({ source: tokeniser.source, tokens: { base } });\n    if (tokeniser.probe(\"<\"))\n      tokeniser.error(`Unsupported generic type ${base.value}`);\n  }\n  if (ret.generic === \"Promise\" && tokeniser.probe(\"?\")) {\n    tokeniser.error(\"Promise type cannot be nullable\");\n  }\n  ret.type = typeName || null;\n  type_suffix(tokeniser, ret);\n  if (ret.nullable && ret.idlType === \"any\")\n    tokeniser.error(\"Type `any` cannot be made nullable\");\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {string} type\n */\nfunction union_type(tokeniser, type) {\n  const tokens = {};\n  tokens.open = tokeniser.consume(\"(\");\n  if (!tokens.open) return;\n  const ret = autoParenter(new Type({ source: tokeniser.source, tokens }));\n  ret.type = type || null;\n  while (true) {\n    const typ =\n      type_with_extended_attributes(tokeniser) ||\n      tokeniser.error(\"No type after open parenthesis or 'or' in union type\");\n    if (typ.idlType === \"any\")\n      tokeniser.error(\"Type `any` cannot be included in a union type\");\n    if (typ.generic === \"Promise\")\n      tokeniser.error(\"Type `Promise` cannot be included in a union type\");\n    ret.subtype.push(typ);\n    const or = tokeniser.consume(\"or\");\n    if (or) {\n      typ.tokens.separator = or;\n    } else break;\n  }\n  if (ret.idlType.length < 2) {\n    tokeniser.error(\n      \"At least two types are expected in a union type but found less\"\n    );\n  }\n  tokens.close =\n    tokeniser.consume(\")\") || tokeniser.error(\"Unterminated union type\");\n  type_suffix(tokeniser, ret);\n  return ret.this;\n}\n\nexport class Type extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {string} typeName\n   */\n  static parse(tokeniser, typeName) {\n    return single_type(tokeniser, typeName) || union_type(tokeniser, typeName);\n  }\n\n  constructor({ source, tokens }) {\n    super({ source, tokens });\n    Object.defineProperty(this, \"subtype\", { value: [], writable: true });\n    this.extAttrs = new ExtendedAttributes({ source, tokens: {} });\n  }\n\n  get generic() {\n    if (this.subtype.length && this.tokens.base) {\n      return this.tokens.base.value;\n    }\n    return \"\";\n  }\n  get nullable() {\n    return Boolean(this.tokens.nullable);\n  }\n  get union() {\n    return Boolean(this.subtype.length) && !this.tokens.base;\n  }\n  get idlType() {\n    if (this.subtype.length) {\n      return this.subtype;\n    }\n    // Adding prefixes/postfixes for \"unrestricted float\", etc.\n    const name = [this.tokens.prefix, this.tokens.base, this.tokens.postfix]\n      .filter((t) => t)\n      .map((t) => t.value)\n      .join(\" \");\n    return unescape(name);\n  }\n\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n\n    if (this.idlType === \"void\") {\n      const message = `\\`void\\` is now replaced by \\`undefined\\`. Refer to the \\\n[relevant GitHub issue](https://github.com/whatwg/webidl/issues/60) \\\nfor more information.`;\n      yield validationError(this.tokens.base, this, \"replace-void\", message, {\n        autofix: replaceVoid(this),\n      });\n    }\n\n    /*\n     * If a union is nullable, its subunions cannot include a dictionary\n     * If not, subunions may include dictionaries if each union is not nullable\n     */\n    const typedef = !this.union && defs.unique.get(this.idlType);\n    const target = this.union\n      ? this\n      : typedef && typedef.type === \"typedef\"\n      ? typedef.idlType\n      : undefined;\n    if (target && this.nullable) {\n      // do not allow any dictionary\n      const { reference } = idlTypeIncludesDictionary(target, defs) || {};\n      if (reference) {\n        const targetToken = (this.union ? reference : this).tokens.base;\n        const message = \"Nullable union cannot include a dictionary type.\";\n        yield validationError(\n          targetToken,\n          this,\n          \"no-nullable-union-dict\",\n          message\n        );\n      }\n    } else {\n      // allow some dictionary\n      for (const subtype of this.subtype) {\n        yield* subtype.validate(defs);\n      }\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const type_body = () => {\n      if (this.union || this.generic) {\n        return w.ts.wrap([\n          w.token(this.tokens.base, w.ts.generic),\n          w.token(this.tokens.open),\n          ...this.subtype.map((t) => t.write(w)),\n          w.token(this.tokens.close),\n        ]);\n      }\n      const firstToken = this.tokens.prefix || this.tokens.base;\n      const prefix = this.tokens.prefix\n        ? [this.tokens.prefix.value, w.ts.trivia(this.tokens.base.trivia)]\n        : [];\n      const ref = w.reference(\n        w.ts.wrap([\n          ...prefix,\n          this.tokens.base.value,\n          w.token(this.tokens.postfix),\n        ]),\n        {\n          unescaped: /** @type {string} (because it's not union) */ (\n            this.idlType\n          ),\n          context: this,\n        }\n      );\n      return w.ts.wrap([w.ts.trivia(firstToken.trivia), ref]);\n    };\n    return w.ts.wrap([\n      this.extAttrs.write(w),\n      type_body(),\n      w.token(this.tokens.nullable),\n      w.token(this.tokens.separator),\n    ]);\n  }\n}\n\n/**\n * @param {Type} type\n */\nfunction replaceVoid(type) {\n  return () => {\n    type.tokens.base.value = \"undefined\";\n  };\n}\n","import { Base } from \"./base.js\";\nimport { const_data, const_value } from \"./helpers.js\";\n\nexport class Default extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const assign = tokeniser.consume(\"=\");\n    if (!assign) {\n      return null;\n    }\n    const def =\n      const_value(tokeniser) ||\n      tokeniser.consumeKind(\"string\") ||\n      tokeniser.consume(\"null\", \"[\", \"{\") ||\n      tokeniser.error(\"No value for default\");\n    const expression = [def];\n    if (def.value === \"[\") {\n      const close =\n        tokeniser.consume(\"]\") ||\n        tokeniser.error(\"Default sequence value must be empty\");\n      expression.push(close);\n    } else if (def.value === \"{\") {\n      const close =\n        tokeniser.consume(\"}\") ||\n        tokeniser.error(\"Default dictionary value must be empty\");\n      expression.push(close);\n    }\n    return new Default({\n      source: tokeniser.source,\n      tokens: { assign },\n      expression,\n    });\n  }\n\n  constructor({ source, tokens, expression }) {\n    super({ source, tokens });\n    expression.parent = this;\n    Object.defineProperty(this, \"expression\", { value: expression });\n  }\n\n  get type() {\n    return const_data(this.expression[0]).type;\n  }\n  get value() {\n    return const_data(this.expression[0]).value;\n  }\n  get negative() {\n    return const_data(this.expression[0]).negative;\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.wrap([\n      w.token(this.tokens.assign),\n      ...this.expression.map((t) => w.token(t)),\n    ]);\n  }\n}\n","import { Base } from \"./base.js\";\nimport { Default } from \"./default.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport {\n  unescape,\n  type_with_extended_attributes,\n  autoParenter,\n  getFirstToken,\n} from \"./helpers.js\";\nimport { argumentNameKeywords, Tokeniser } from \"../tokeniser.js\";\nimport { validationError } from \"../error.js\";\nimport {\n  idlTypeIncludesDictionary,\n  dictionaryIncludesRequiredField,\n} from \"../validators/helpers.js\";\n\nexport class Argument extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const start_position = tokeniser.position;\n    /** @type {Base[\"tokens\"]} */\n    const tokens = {};\n    const ret = autoParenter(\n      new Argument({ source: tokeniser.source, tokens })\n    );\n    ret.extAttrs = ExtendedAttributes.parse(tokeniser);\n    tokens.optional = tokeniser.consume(\"optional\");\n    ret.idlType = type_with_extended_attributes(tokeniser, \"argument-type\");\n    if (!ret.idlType) {\n      return tokeniser.unconsume(start_position);\n    }\n    if (!tokens.optional) {\n      tokens.variadic = tokeniser.consume(\"...\");\n    }\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.consume(...argumentNameKeywords);\n    if (!tokens.name) {\n      return tokeniser.unconsume(start_position);\n    }\n    ret.default = tokens.optional ? Default.parse(tokeniser) : null;\n    return ret.this;\n  }\n\n  get type() {\n    return \"argument\";\n  }\n  get optional() {\n    return !!this.tokens.optional;\n  }\n  get variadic() {\n    return !!this.tokens.variadic;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  /**\n   * @param {import(\"../validator.js\").Definitions} defs\n   */\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n    yield* this.idlType.validate(defs);\n    const result = idlTypeIncludesDictionary(this.idlType, defs, {\n      useNullableInner: true,\n    });\n    if (result) {\n      if (this.idlType.nullable) {\n        const message = `Dictionary arguments cannot be nullable.`;\n        yield validationError(\n          this.tokens.name,\n          this,\n          \"no-nullable-dict-arg\",\n          message\n        );\n      } else if (!this.optional) {\n        if (\n          this.parent &&\n          !dictionaryIncludesRequiredField(result.dictionary, defs) &&\n          isLastRequiredArgument(this)\n        ) {\n          const message = `Dictionary argument must be optional if it has no required fields`;\n          yield validationError(\n            this.tokens.name,\n            this,\n            \"dict-arg-optional\",\n            message,\n            {\n              autofix: autofixDictionaryArgumentOptionality(this),\n            }\n          );\n        }\n      } else if (!this.default) {\n        const message = `Optional dictionary arguments must have a default value of \\`{}\\`.`;\n        yield validationError(\n          this.tokens.name,\n          this,\n          \"dict-arg-default\",\n          message,\n          {\n            autofix: autofixOptionalDictionaryDefaultValue(this),\n          }\n        );\n      }\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.wrap([\n      this.extAttrs.write(w),\n      w.token(this.tokens.optional),\n      w.ts.type(this.idlType.write(w)),\n      w.token(this.tokens.variadic),\n      w.name_token(this.tokens.name, { data: this }),\n      this.default ? this.default.write(w) : \"\",\n      w.token(this.tokens.separator),\n    ]);\n  }\n}\n\n/**\n * @param {Argument} arg\n */\nfunction isLastRequiredArgument(arg) {\n  const list = arg.parent.arguments || arg.parent.list;\n  const index = list.indexOf(arg);\n  const requiredExists = list.slice(index + 1).some((a) => !a.optional);\n  return !requiredExists;\n}\n\n/**\n * @param {Argument} arg\n */\nfunction autofixDictionaryArgumentOptionality(arg) {\n  return () => {\n    const firstToken = getFirstToken(arg.idlType);\n    arg.tokens.optional = {\n      ...firstToken,\n      type: \"optional\",\n      value: \"optional\",\n    };\n    firstToken.trivia = \" \";\n    autofixOptionalDictionaryDefaultValue(arg)();\n  };\n}\n\n/**\n * @param {Argument} arg\n */\nfunction autofixOptionalDictionaryDefaultValue(arg) {\n  return () => {\n    arg.default = Default.parse(new Tokeniser(\" = {}\"));\n  };\n}\n","import { Base } from \"./base.js\";\nimport {\n  return_type,\n  argument_list,\n  unescape,\n  autoParenter,\n} from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\n\nexport class Operation extends Base {\n  /**\n   * @typedef {import(\"../tokeniser.js\").Token} Token\n   *\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {object} [options]\n   * @param {Token} [options.special]\n   * @param {Token} [options.regular]\n   */\n  static parse(tokeniser, { special, regular } = {}) {\n    const tokens = { special };\n    const ret = autoParenter(\n      new Operation({ source: tokeniser.source, tokens })\n    );\n    if (special && special.value === \"stringifier\") {\n      tokens.termination = tokeniser.consume(\";\");\n      if (tokens.termination) {\n        ret.arguments = [];\n        return ret;\n      }\n    }\n    if (!special && !regular) {\n      tokens.special = tokeniser.consume(\"getter\", \"setter\", \"deleter\");\n    }\n    ret.idlType =\n      return_type(tokeniser) || tokeniser.error(\"Missing return type\");\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") || tokeniser.consume(\"includes\");\n    tokens.open =\n      tokeniser.consume(\"(\") || tokeniser.error(\"Invalid operation\");\n    ret.arguments = argument_list(tokeniser);\n    tokens.close =\n      tokeniser.consume(\")\") || tokeniser.error(\"Unterminated operation\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"Unterminated operation, expected `;`\");\n    return ret.this;\n  }\n\n  get type() {\n    return \"operation\";\n  }\n  get name() {\n    const { name } = this.tokens;\n    if (!name) {\n      return \"\";\n    }\n    return unescape(name.value);\n  }\n  get special() {\n    if (!this.tokens.special) {\n      return \"\";\n    }\n    return this.tokens.special.value;\n  }\n\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n    if (!this.name && [\"\", \"static\"].includes(this.special)) {\n      const message = `Regular or static operations must have both a return type and an identifier.`;\n      yield validationError(this.tokens.open, this, \"incomplete-op\", message);\n    }\n    if (this.idlType) {\n      yield* this.idlType.validate(defs);\n    }\n    for (const argument of this.arguments) {\n      yield* argument.validate(defs);\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { parent } = this;\n    const body = this.idlType\n      ? [\n          w.ts.type(this.idlType.write(w)),\n          w.name_token(this.tokens.name, { data: this, parent }),\n          w.token(this.tokens.open),\n          w.ts.wrap(this.arguments.map((arg) => arg.write(w))),\n          w.token(this.tokens.close),\n        ]\n      : [];\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        this.tokens.name\n          ? w.token(this.tokens.special)\n          : w.token(this.tokens.special, w.ts.nameless, { data: this, parent }),\n        ...body,\n        w.token(this.tokens.termination),\n      ]),\n      { data: this, parent }\n    );\n  }\n}\n","import { validationError } from \"../error.js\";\nimport { idlTypeIncludesDictionary } from \"../validators/helpers.js\";\nimport { Base } from \"./base.js\";\nimport {\n  type_with_extended_attributes,\n  unescape,\n  autoParenter,\n} from \"./helpers.js\";\n\nexport class Attribute extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {object} [options]\n   * @param {import(\"../tokeniser.js\").Token} [options.special]\n   * @param {boolean} [options.noInherit]\n   * @param {boolean} [options.readonly]\n   */\n  static parse(\n    tokeniser,\n    { special, noInherit = false, readonly = false } = {}\n  ) {\n    const start_position = tokeniser.position;\n    const tokens = { special };\n    const ret = autoParenter(\n      new Attribute({ source: tokeniser.source, tokens })\n    );\n    if (!special && !noInherit) {\n      tokens.special = tokeniser.consume(\"inherit\");\n    }\n    if (ret.special === \"inherit\" && tokeniser.probe(\"readonly\")) {\n      tokeniser.error(\"Inherited attributes cannot be read-only\");\n    }\n    tokens.readonly = tokeniser.consume(\"readonly\");\n    if (readonly && !tokens.readonly && tokeniser.probe(\"attribute\")) {\n      tokeniser.error(\"Attributes must be readonly in this context\");\n    }\n    tokens.base = tokeniser.consume(\"attribute\");\n    if (!tokens.base) {\n      tokeniser.unconsume(start_position);\n      return;\n    }\n    ret.idlType =\n      type_with_extended_attributes(tokeniser, \"attribute-type\") ||\n      tokeniser.error(\"Attribute lacks a type\");\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.consume(\"async\", \"required\") ||\n      tokeniser.error(\"Attribute lacks a name\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"Unterminated attribute, expected `;`\");\n    return ret.this;\n  }\n\n  get type() {\n    return \"attribute\";\n  }\n  get special() {\n    if (!this.tokens.special) {\n      return \"\";\n    }\n    return this.tokens.special.value;\n  }\n  get readonly() {\n    return !!this.tokens.readonly;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n    yield* this.idlType.validate(defs);\n\n    switch (this.idlType.generic) {\n      case \"sequence\":\n      case \"record\": {\n        const message = `Attributes cannot accept ${this.idlType.generic} types.`;\n        yield validationError(\n          this.tokens.name,\n          this,\n          \"attr-invalid-type\",\n          message\n        );\n        break;\n      }\n      default: {\n        const { reference } =\n          idlTypeIncludesDictionary(this.idlType, defs) || {};\n        if (reference) {\n          const targetToken = (this.idlType.union ? reference : this.idlType)\n            .tokens.base;\n          const message = \"Attributes cannot accept dictionary types.\";\n          yield validationError(\n            targetToken,\n            this,\n            \"attr-invalid-type\",\n            message\n          );\n        }\n      }\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { parent } = this;\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.special),\n        w.token(this.tokens.readonly),\n        w.token(this.tokens.base),\n        w.ts.type(this.idlType.write(w)),\n        w.name_token(this.tokens.name, { data: this, parent }),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this, parent }\n    );\n  }\n}\n","import { Type } from \"./type.js\";\nimport { Argument } from \"./argument.js\";\nimport {\n  ExtendedAttributes,\n  SimpleExtendedAttribute,\n} from \"./extended-attributes.js\";\nimport { Operation } from \"./operation.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Tokeniser } from \"../tokeniser.js\";\n\n/**\n * @param {string} identifier\n */\nexport function unescape(identifier) {\n  return identifier.startsWith(\"_\") ? identifier.slice(1) : identifier;\n}\n\n/**\n * Parses comma-separated list\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {object} args\n * @param {Function} args.parser parser function for each item\n * @param {boolean} [args.allowDangler] whether to allow dangling comma\n * @param {string} [args.listName] the name to be shown on error messages\n */\nexport function list(tokeniser, { parser, allowDangler, listName = \"list\" }) {\n  const first = parser(tokeniser);\n  if (!first) {\n    return [];\n  }\n  first.tokens.separator = tokeniser.consume(\",\");\n  const items = [first];\n  while (first.tokens.separator) {\n    const item = parser(tokeniser);\n    if (!item) {\n      if (!allowDangler) {\n        tokeniser.error(`Trailing comma in ${listName}`);\n      }\n      break;\n    }\n    item.tokens.separator = tokeniser.consume(\",\");\n    items.push(item);\n    if (!item.tokens.separator) break;\n  }\n  return items;\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nexport function const_value(tokeniser) {\n  return (\n    tokeniser.consumeKind(\"decimal\", \"integer\") ||\n    tokeniser.consume(\"true\", \"false\", \"Infinity\", \"-Infinity\", \"NaN\")\n  );\n}\n\n/**\n * @param {object} token\n * @param {string} token.type\n * @param {string} token.value\n */\nexport function const_data({ type, value }) {\n  switch (type) {\n    case \"decimal\":\n    case \"integer\":\n      return { type: \"number\", value };\n    case \"string\":\n      return { type: \"string\", value: value.slice(1, -1) };\n  }\n\n  switch (value) {\n    case \"true\":\n    case \"false\":\n      return { type: \"boolean\", value: value === \"true\" };\n    case \"Infinity\":\n    case \"-Infinity\":\n      return { type: \"Infinity\", negative: value.startsWith(\"-\") };\n    case \"[\":\n      return { type: \"sequence\", value: [] };\n    case \"{\":\n      return { type: \"dictionary\" };\n    default:\n      return { type: value };\n  }\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nexport function primitive_type(tokeniser) {\n  function integer_type() {\n    const prefix = tokeniser.consume(\"unsigned\");\n    const base = tokeniser.consume(\"short\", \"long\");\n    if (base) {\n      const postfix = tokeniser.consume(\"long\");\n      return new Type({ source, tokens: { prefix, base, postfix } });\n    }\n    if (prefix) tokeniser.error(\"Failed to parse integer type\");\n  }\n\n  function decimal_type() {\n    const prefix = tokeniser.consume(\"unrestricted\");\n    const base = tokeniser.consume(\"float\", \"double\");\n    if (base) {\n      return new Type({ source, tokens: { prefix, base } });\n    }\n    if (prefix) tokeniser.error(\"Failed to parse float type\");\n  }\n\n  const { source } = tokeniser;\n  const num_type = integer_type() || decimal_type();\n  if (num_type) return num_type;\n  const base = tokeniser.consume(\n    \"bigint\",\n    \"boolean\",\n    \"byte\",\n    \"octet\",\n    \"undefined\"\n  );\n  if (base) {\n    return new Type({ source, tokens: { base } });\n  }\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nexport function argument_list(tokeniser) {\n  return list(tokeniser, {\n    parser: Argument.parse,\n    listName: \"arguments list\",\n  });\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {string=} typeName (TODO: See Type.type for more details)\n */\nexport function type_with_extended_attributes(tokeniser, typeName) {\n  const extAttrs = ExtendedAttributes.parse(tokeniser);\n  const ret = Type.parse(tokeniser, typeName);\n  if (ret) autoParenter(ret).extAttrs = extAttrs;\n  return ret;\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n * @param {string=} typeName (TODO: See Type.type for more details)\n */\nexport function return_type(tokeniser, typeName) {\n  const typ = Type.parse(tokeniser, typeName || \"return-type\");\n  if (typ) {\n    return typ;\n  }\n  const voidToken = tokeniser.consume(\"void\");\n  if (voidToken) {\n    const ret = new Type({\n      source: tokeniser.source,\n      tokens: { base: voidToken },\n    });\n    ret.type = \"return-type\";\n    return ret;\n  }\n}\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nexport function stringifier(tokeniser) {\n  const special = tokeniser.consume(\"stringifier\");\n  if (!special) return;\n  const member =\n    Attribute.parse(tokeniser, { special }) ||\n    Operation.parse(tokeniser, { special }) ||\n    tokeniser.error(\"Unterminated stringifier\");\n  return member;\n}\n\n/**\n * @param {string} str\n */\nexport function getLastIndentation(str) {\n  const lines = str.split(\"\\n\");\n  // the first line visually binds to the preceding token\n  if (lines.length) {\n    const match = lines[lines.length - 1].match(/^\\s+/);\n    if (match) {\n      return match[0];\n    }\n  }\n  return \"\";\n}\n\n/**\n * @param {string} parentTrivia\n */\nexport function getMemberIndentation(parentTrivia) {\n  const indentation = getLastIndentation(parentTrivia);\n  const indentCh = indentation.includes(\"\\t\") ? \"\\t\" : \"  \";\n  return indentation + indentCh;\n}\n\n/**\n * @param {import(\"./interface.js\").Interface} def\n */\nexport function autofixAddExposedWindow(def) {\n  return () => {\n    if (def.extAttrs.length) {\n      const tokeniser = new Tokeniser(\"Exposed=Window,\");\n      const exposed = SimpleExtendedAttribute.parse(tokeniser);\n      exposed.tokens.separator = tokeniser.consume(\",\");\n      const existing = def.extAttrs[0];\n      if (!/^\\s/.test(existing.tokens.name.trivia)) {\n        existing.tokens.name.trivia = ` ${existing.tokens.name.trivia}`;\n      }\n      def.extAttrs.unshift(exposed);\n    } else {\n      autoParenter(def).extAttrs = ExtendedAttributes.parse(\n        new Tokeniser(\"[Exposed=Window]\")\n      );\n      const trivia = def.tokens.base.trivia;\n      def.extAttrs.tokens.open.trivia = trivia;\n      def.tokens.base.trivia = `\\n${getLastIndentation(trivia)}`;\n    }\n  };\n}\n\n/**\n * Get the first syntax token for the given IDL object.\n * @param {*} data\n */\nexport function getFirstToken(data) {\n  if (data.extAttrs.length) {\n    return data.extAttrs.tokens.open;\n  }\n  if (data.type === \"operation\" && !data.special) {\n    return getFirstToken(data.idlType);\n  }\n  const tokens = Object.values(data.tokens).sort((x, y) => x.index - y.index);\n  return tokens[0];\n}\n\n/**\n * @template T\n * @param {T[]} array\n * @param {(item: T) => boolean} predicate\n */\nexport function findLastIndex(array, predicate) {\n  const index = array.slice().reverse().findIndex(predicate);\n  if (index === -1) {\n    return index;\n  }\n  return array.length - index - 1;\n}\n\n/**\n * Returns a proxy that auto-assign `parent` field.\n * @template {Record<string | symbol, any>} T\n * @param {T} data\n * @param {*} [parent] The object that will be assigned to `parent`.\n *                     If absent, it will be `data` by default.\n * @return {T}\n */\nexport function autoParenter(data, parent) {\n  if (!parent) {\n    // Defaults to `data` unless specified otherwise.\n    parent = data;\n  }\n  if (!data) {\n    // This allows `autoParenter(undefined)` which again allows\n    // `autoParenter(parse())` where the function may return nothing.\n    return data;\n  }\n  const proxy = new Proxy(data, {\n    get(target, p) {\n      const value = target[p];\n      if (Array.isArray(value) && p !== \"source\") {\n        // Wraps the array so that any added items will also automatically\n        // get their `parent` values.\n        return autoParenter(value, target);\n      }\n      return value;\n    },\n    set(target, p, value) {\n      // @ts-ignore https://github.com/microsoft/TypeScript/issues/47357\n      target[p] = value;\n      if (!value) {\n        return true;\n      } else if (Array.isArray(value)) {\n        // Assigning an array will add `parent` to its items.\n        for (const item of value) {\n          if (typeof item.parent !== \"undefined\") {\n            item.parent = parent;\n          }\n        }\n      } else if (typeof value.parent !== \"undefined\") {\n        value.parent = parent;\n      }\n      return true;\n    },\n  });\n  return proxy;\n}\n","import { syntaxError } from \"./error.js\";\nimport { unescape } from \"./productions/helpers.js\";\n\n// These regular expressions use the sticky flag so they will only match at\n// the current location (ie. the offset of lastIndex).\nconst tokenRe = {\n  // This expression uses a lookahead assertion to catch false matches\n  // against integers early.\n  decimal:\n    /-?(?=[0-9]*\\.|[0-9]+[eE])(([0-9]+\\.[0-9]*|[0-9]*\\.[0-9]+)([Ee][-+]?[0-9]+)?|[0-9]+[Ee][-+]?[0-9]+)/y,\n  integer: /-?(0([Xx][0-9A-Fa-f]+|[0-7]*)|[1-9][0-9]*)/y,\n  identifier: /[_-]?[A-Za-z][0-9A-Z_a-z-]*/y,\n  string: /\"[^\"]*\"/y,\n  whitespace: /[\\t\\n\\r ]+/y,\n  comment: /\\/\\/.*|\\/\\*[\\s\\S]*?\\*\\//y,\n  other: /[^\\t\\n\\r 0-9A-Za-z]/y,\n};\n\nexport const typeNameKeywords = [\n  \"ArrayBuffer\",\n  \"DataView\",\n  \"Int8Array\",\n  \"Int16Array\",\n  \"Int32Array\",\n  \"Uint8Array\",\n  \"Uint16Array\",\n  \"Uint32Array\",\n  \"Uint8ClampedArray\",\n  \"BigInt64Array\",\n  \"BigUint64Array\",\n  \"Float32Array\",\n  \"Float64Array\",\n  \"any\",\n  \"object\",\n  \"symbol\",\n];\n\nexport const stringTypes = [\"ByteString\", \"DOMString\", \"USVString\"];\n\nexport const argumentNameKeywords = [\n  \"async\",\n  \"attribute\",\n  \"callback\",\n  \"const\",\n  \"constructor\",\n  \"deleter\",\n  \"dictionary\",\n  \"enum\",\n  \"getter\",\n  \"includes\",\n  \"inherit\",\n  \"interface\",\n  \"iterable\",\n  \"maplike\",\n  \"namespace\",\n  \"partial\",\n  \"required\",\n  \"setlike\",\n  \"setter\",\n  \"static\",\n  \"stringifier\",\n  \"typedef\",\n  \"unrestricted\",\n];\n\nconst nonRegexTerminals = [\n  \"-Infinity\",\n  \"FrozenArray\",\n  \"Infinity\",\n  \"NaN\",\n  \"ObservableArray\",\n  \"Promise\",\n  \"bigint\",\n  \"boolean\",\n  \"byte\",\n  \"double\",\n  \"false\",\n  \"float\",\n  \"long\",\n  \"mixin\",\n  \"null\",\n  \"octet\",\n  \"optional\",\n  \"or\",\n  \"readonly\",\n  \"record\",\n  \"sequence\",\n  \"short\",\n  \"true\",\n  \"undefined\",\n  \"unsigned\",\n  \"void\",\n].concat(argumentNameKeywords, stringTypes, typeNameKeywords);\n\nconst punctuations = [\n  \"(\",\n  \")\",\n  \",\",\n  \"...\",\n  \":\",\n  \";\",\n  \"<\",\n  \"=\",\n  \">\",\n  \"?\",\n  \"*\",\n  \"[\",\n  \"]\",\n  \"{\",\n  \"}\",\n];\n\nconst reserved = [\n  // \"constructor\" is now a keyword\n  \"_constructor\",\n  \"toString\",\n  \"_toString\",\n];\n\n/**\n * @typedef {ArrayItemType<ReturnType<typeof tokenise>>} Token\n * @param {string} str\n */\nfunction tokenise(str) {\n  const tokens = [];\n  let lastCharIndex = 0;\n  let trivia = \"\";\n  let line = 1;\n  let index = 0;\n  while (lastCharIndex < str.length) {\n    const nextChar = str.charAt(lastCharIndex);\n    let result = -1;\n\n    if (/[\\t\\n\\r ]/.test(nextChar)) {\n      result = attemptTokenMatch(\"whitespace\", { noFlushTrivia: true });\n    } else if (nextChar === \"/\") {\n      result = attemptTokenMatch(\"comment\", { noFlushTrivia: true });\n    }\n\n    if (result !== -1) {\n      const currentTrivia = tokens.pop().value;\n      line += (currentTrivia.match(/\\n/g) || []).length;\n      trivia += currentTrivia;\n      index -= 1;\n    } else if (/[-0-9.A-Z_a-z]/.test(nextChar)) {\n      result = attemptTokenMatch(\"decimal\");\n      if (result === -1) {\n        result = attemptTokenMatch(\"integer\");\n      }\n      if (result === -1) {\n        result = attemptTokenMatch(\"identifier\");\n        const lastIndex = tokens.length - 1;\n        const token = tokens[lastIndex];\n        if (result !== -1) {\n          if (reserved.includes(token.value)) {\n            const message = `${unescape(\n              token.value\n            )} is a reserved identifier and must not be used.`;\n            throw new WebIDLParseError(\n              syntaxError(tokens, lastIndex, null, message)\n            );\n          } else if (nonRegexTerminals.includes(token.value)) {\n            token.type = \"inline\";\n          }\n        }\n      }\n    } else if (nextChar === '\"') {\n      result = attemptTokenMatch(\"string\");\n    }\n\n    for (const punctuation of punctuations) {\n      if (str.startsWith(punctuation, lastCharIndex)) {\n        tokens.push({\n          type: \"inline\",\n          value: punctuation,\n          trivia,\n          line,\n          index,\n        });\n        trivia = \"\";\n        lastCharIndex += punctuation.length;\n        result = lastCharIndex;\n        break;\n      }\n    }\n\n    // other as the last try\n    if (result === -1) {\n      result = attemptTokenMatch(\"other\");\n    }\n    if (result === -1) {\n      throw new Error(\"Token stream not progressing\");\n    }\n    lastCharIndex = result;\n    index += 1;\n  }\n\n  // remaining trivia as eof\n  tokens.push({\n    type: \"eof\",\n    value: \"\",\n    trivia,\n    line,\n    index,\n  });\n\n  return tokens;\n\n  /**\n   * @param {keyof typeof tokenRe} type\n   * @param {object} options\n   * @param {boolean} [options.noFlushTrivia]\n   */\n  function attemptTokenMatch(type, { noFlushTrivia } = {}) {\n    const re = tokenRe[type];\n    re.lastIndex = lastCharIndex;\n    const result = re.exec(str);\n    if (result) {\n      tokens.push({ type, value: result[0], trivia, line, index });\n      if (!noFlushTrivia) {\n        trivia = \"\";\n      }\n      return re.lastIndex;\n    }\n    return -1;\n  }\n}\n\nexport class Tokeniser {\n  /**\n   * @param {string} idl\n   */\n  constructor(idl) {\n    this.source = tokenise(idl);\n    this.position = 0;\n  }\n\n  /**\n   * @param {string} message\n   * @return {never}\n   */\n  error(message) {\n    throw new WebIDLParseError(\n      syntaxError(this.source, this.position, this.current, message)\n    );\n  }\n\n  /**\n   * @param {string} type\n   */\n  probeKind(type) {\n    return (\n      this.source.length > this.position &&\n      this.source[this.position].type === type\n    );\n  }\n\n  /**\n   * @param {string} value\n   */\n  probe(value) {\n    return (\n      this.probeKind(\"inline\") && this.source[this.position].value === value\n    );\n  }\n\n  /**\n   * @param {...string} candidates\n   */\n  consumeKind(...candidates) {\n    for (const type of candidates) {\n      if (!this.probeKind(type)) continue;\n      const token = this.source[this.position];\n      this.position++;\n      return token;\n    }\n  }\n\n  /**\n   * @param {...string} candidates\n   */\n  consume(...candidates) {\n    if (!this.probeKind(\"inline\")) return;\n    const token = this.source[this.position];\n    for (const value of candidates) {\n      if (token.value !== value) continue;\n      this.position++;\n      return token;\n    }\n  }\n\n  /**\n   * @param {string} value\n   */\n  consumeIdentifier(value) {\n    if (!this.probeKind(\"identifier\")) {\n      return;\n    }\n    if (this.source[this.position].value !== value) {\n      return;\n    }\n    return this.consumeKind(\"identifier\");\n  }\n\n  /**\n   * @param {number} position\n   */\n  unconsume(position) {\n    this.position = position;\n  }\n}\n\nexport class WebIDLParseError extends Error {\n  /**\n   * @param {object} options\n   * @param {string} options.message\n   * @param {string} options.bareMessage\n   * @param {string} options.context\n   * @param {number} options.line\n   * @param {*} options.sourceName\n   * @param {string} options.input\n   * @param {*[]} options.tokens\n   */\n  constructor({\n    message,\n    bareMessage,\n    context,\n    line,\n    sourceName,\n    input,\n    tokens,\n  }) {\n    super(message);\n\n    this.name = \"WebIDLParseError\"; // not to be mangled\n    this.bareMessage = bareMessage;\n    this.context = context;\n    this.line = line;\n    this.sourceName = sourceName;\n    this.input = input;\n    this.tokens = tokens;\n  }\n}\n","import { list, unescape, autoParenter } from \"./helpers.js\";\nimport { WrappedToken } from \"./token.js\";\nimport { Base } from \"./base.js\";\n\nexport class EnumValue extends WrappedToken {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const value = tokeniser.consumeKind(\"string\");\n    if (value) {\n      return new EnumValue({ source: tokeniser.source, tokens: { value } });\n    }\n  }\n\n  get type() {\n    return \"enum-value\";\n  }\n  get value() {\n    return super.value.slice(1, -1);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { parent } = this;\n    return w.ts.wrap([\n      w.ts.trivia(this.tokens.value.trivia),\n      w.ts.definition(\n        w.ts.wrap(['\"', w.ts.name(this.value, { data: this, parent }), '\"']),\n        { data: this, parent }\n      ),\n      w.token(this.tokens.separator),\n    ]);\n  }\n}\n\nexport class Enum extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    /** @type {Base[\"tokens\"]} */\n    const tokens = {};\n    tokens.base = tokeniser.consume(\"enum\");\n    if (!tokens.base) {\n      return;\n    }\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(\"No name for enum\");\n    const ret = autoParenter(new Enum({ source: tokeniser.source, tokens }));\n    tokeniser.current = ret.this;\n    tokens.open = tokeniser.consume(\"{\") || tokeniser.error(\"Bodyless enum\");\n    ret.values = list(tokeniser, {\n      parser: EnumValue.parse,\n      allowDangler: true,\n      listName: \"enumeration\",\n    });\n    if (tokeniser.probeKind(\"string\")) {\n      tokeniser.error(\"No comma between enum values\");\n    }\n    tokens.close =\n      tokeniser.consume(\"}\") || tokeniser.error(\"Unexpected value in enum\");\n    if (!ret.values.length) {\n      tokeniser.error(\"No value in enum\");\n    }\n    tokens.termination =\n      tokeniser.consume(\";\") || tokeniser.error(\"No semicolon after enum\");\n    return ret.this;\n  }\n\n  get type() {\n    return \"enum\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.base),\n        w.name_token(this.tokens.name, { data: this }),\n        w.token(this.tokens.open),\n        w.ts.wrap(this.values.map((v) => v.write(w))),\n        w.token(this.tokens.close),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport { unescape } from \"./helpers.js\";\n\nexport class Includes extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const target = tokeniser.consumeKind(\"identifier\");\n    if (!target) {\n      return;\n    }\n    const tokens = { target };\n    tokens.includes = tokeniser.consume(\"includes\");\n    if (!tokens.includes) {\n      tokeniser.unconsume(target.index);\n      return;\n    }\n    tokens.mixin =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(\"Incomplete includes statement\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"No terminating ; for includes statement\");\n    return new Includes({ source: tokeniser.source, tokens });\n  }\n\n  get type() {\n    return \"includes\";\n  }\n  get target() {\n    return unescape(this.tokens.target.value);\n  }\n  get includes() {\n    return unescape(this.tokens.mixin.value);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.reference_token(this.tokens.target, this),\n        w.token(this.tokens.includes),\n        w.reference_token(this.tokens.mixin, this),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport {\n  type_with_extended_attributes,\n  unescape,\n  autoParenter,\n} from \"./helpers.js\";\n\nexport class Typedef extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    /** @type {Base[\"tokens\"]} */\n    const tokens = {};\n    const ret = autoParenter(new Typedef({ source: tokeniser.source, tokens }));\n    tokens.base = tokeniser.consume(\"typedef\");\n    if (!tokens.base) {\n      return;\n    }\n    ret.idlType =\n      type_with_extended_attributes(tokeniser, \"typedef-type\") ||\n      tokeniser.error(\"Typedef lacks a type\");\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(\"Typedef lacks a name\");\n    tokeniser.current = ret.this;\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"Unterminated typedef, expected `;`\");\n    return ret.this;\n  }\n\n  get type() {\n    return \"typedef\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.base),\n        w.ts.type(this.idlType.write(w)),\n        w.name_token(this.tokens.name, { data: this }),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport {\n  return_type,\n  argument_list,\n  unescape,\n  autoParenter,\n} from \"./helpers.js\";\n\nexport class CallbackFunction extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base) {\n    const tokens = { base };\n    const ret = autoParenter(\n      new CallbackFunction({ source: tokeniser.source, tokens })\n    );\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(\"Callback lacks a name\");\n    tokeniser.current = ret.this;\n    tokens.assign =\n      tokeniser.consume(\"=\") || tokeniser.error(\"Callback lacks an assignment\");\n    ret.idlType =\n      return_type(tokeniser) || tokeniser.error(\"Callback lacks a return type\");\n    tokens.open =\n      tokeniser.consume(\"(\") ||\n      tokeniser.error(\"Callback lacks parentheses for arguments\");\n    ret.arguments = argument_list(tokeniser);\n    tokens.close =\n      tokeniser.consume(\")\") || tokeniser.error(\"Unterminated callback\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"Unterminated callback, expected `;`\");\n    return ret.this;\n  }\n\n  get type() {\n    return \"callback\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n    yield* this.idlType.validate(defs);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.base),\n        w.name_token(this.tokens.name, { data: this }),\n        w.token(this.tokens.assign),\n        w.ts.type(this.idlType.write(w)),\n        w.token(this.tokens.open),\n        ...this.arguments.map((arg) => arg.write(w)),\n        w.token(this.tokens.close),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { unescape, autoParenter } from \"./helpers.js\";\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nfunction inheritance(tokeniser) {\n  const colon = tokeniser.consume(\":\");\n  if (!colon) {\n    return {};\n  }\n  const inheritance =\n    tokeniser.consumeKind(\"identifier\") ||\n    tokeniser.error(\"Inheritance lacks a type\");\n  return { colon, inheritance };\n}\n\nexport class Container extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {*} instance TODO: This should be {T extends Container}, but see https://github.com/microsoft/TypeScript/issues/4628\n   * @param {*} args\n   */\n  static parse(tokeniser, instance, { inheritable, allowedMembers }) {\n    const { tokens, type } = instance;\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(`Missing name in ${type}`);\n    tokeniser.current = instance;\n    instance = autoParenter(instance);\n    if (inheritable) {\n      Object.assign(tokens, inheritance(tokeniser));\n    }\n    tokens.open = tokeniser.consume(\"{\") || tokeniser.error(`Bodyless ${type}`);\n    instance.members = [];\n    while (true) {\n      tokens.close = tokeniser.consume(\"}\");\n      if (tokens.close) {\n        tokens.termination =\n          tokeniser.consume(\";\") ||\n          tokeniser.error(`Missing semicolon after ${type}`);\n        return instance.this;\n      }\n      const ea = ExtendedAttributes.parse(tokeniser);\n      let mem;\n      for (const [parser, ...args] of allowedMembers) {\n        mem = autoParenter(parser(tokeniser, ...args));\n        if (mem) {\n          break;\n        }\n      }\n      if (!mem) {\n        tokeniser.error(\"Unknown member\");\n      }\n      mem.extAttrs = ea;\n      instance.members.push(mem.this);\n    }\n  }\n\n  get partial() {\n    return !!this.tokens.partial;\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get inheritance() {\n    if (!this.tokens.inheritance) {\n      return null;\n    }\n    return unescape(this.tokens.inheritance.value);\n  }\n\n  *validate(defs) {\n    for (const member of this.members) {\n      if (member.validate) {\n        yield* member.validate(defs);\n      }\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const inheritance = () => {\n      if (!this.tokens.inheritance) {\n        return \"\";\n      }\n      return w.ts.wrap([\n        w.token(this.tokens.colon),\n        w.ts.trivia(this.tokens.inheritance.trivia),\n        w.ts.inheritance(\n          w.reference(this.tokens.inheritance.value, { context: this })\n        ),\n      ]);\n    };\n\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.callback),\n        w.token(this.tokens.partial),\n        w.token(this.tokens.base),\n        w.token(this.tokens.mixin),\n        w.name_token(this.tokens.name, { data: this }),\n        inheritance(),\n        w.token(this.tokens.open),\n        w.ts.wrap(this.members.map((m) => m.write(w))),\n        w.token(this.tokens.close),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport { Type } from \"./type.js\";\nimport {\n  const_data,\n  const_value,\n  primitive_type,\n  autoParenter,\n  unescape,\n} from \"./helpers.js\";\n\nexport class Constant extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    /** @type {Base[\"tokens\"]} */\n    const tokens = {};\n    tokens.base = tokeniser.consume(\"const\");\n    if (!tokens.base) {\n      return;\n    }\n    let idlType = primitive_type(tokeniser);\n    if (!idlType) {\n      const base =\n        tokeniser.consumeKind(\"identifier\") ||\n        tokeniser.error(\"Const lacks a type\");\n      idlType = new Type({ source: tokeniser.source, tokens: { base } });\n    }\n    if (tokeniser.probe(\"?\")) {\n      tokeniser.error(\"Unexpected nullable constant type\");\n    }\n    idlType.type = \"const-type\";\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(\"Const lacks a name\");\n    tokens.assign =\n      tokeniser.consume(\"=\") || tokeniser.error(\"Const lacks value assignment\");\n    tokens.value =\n      const_value(tokeniser) || tokeniser.error(\"Const lacks a value\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"Unterminated const, expected `;`\");\n    const ret = new Constant({ source: tokeniser.source, tokens });\n    autoParenter(ret).idlType = idlType;\n    return ret;\n  }\n\n  get type() {\n    return \"const\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get value() {\n    return const_data(this.tokens.value);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { parent } = this;\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.base),\n        w.ts.type(this.idlType.write(w)),\n        w.name_token(this.tokens.name, { data: this, parent }),\n        w.token(this.tokens.assign),\n        w.token(this.tokens.value),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this, parent }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport {\n  type_with_extended_attributes,\n  autoParenter,\n  argument_list,\n} from \"./helpers.js\";\n\nexport class IterableLike extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const start_position = tokeniser.position;\n    const ret = autoParenter(\n      new IterableLike({ source: tokeniser.source, tokens: {} })\n    );\n    const { tokens } = ret;\n    tokens.readonly = tokeniser.consume(\"readonly\");\n    if (!tokens.readonly) {\n      tokens.async = tokeniser.consume(\"async\");\n    }\n    tokens.base = tokens.readonly\n      ? tokeniser.consume(\"maplike\", \"setlike\")\n      : tokens.async\n      ? tokeniser.consume(\"iterable\")\n      : tokeniser.consume(\"iterable\", \"maplike\", \"setlike\");\n    if (!tokens.base) {\n      tokeniser.unconsume(start_position);\n      return;\n    }\n\n    const { type } = ret;\n    const secondTypeRequired = type === \"maplike\";\n    const secondTypeAllowed = secondTypeRequired || type === \"iterable\";\n    const argumentAllowed = ret.async && type === \"iterable\";\n\n    tokens.open =\n      tokeniser.consume(\"<\") ||\n      tokeniser.error(`Missing less-than sign \\`<\\` in ${type} declaration`);\n    const first =\n      type_with_extended_attributes(tokeniser) ||\n      tokeniser.error(`Missing a type argument in ${type} declaration`);\n    ret.idlType = [first];\n    ret.arguments = [];\n\n    if (secondTypeAllowed) {\n      first.tokens.separator = tokeniser.consume(\",\");\n      if (first.tokens.separator) {\n        ret.idlType.push(type_with_extended_attributes(tokeniser));\n      } else if (secondTypeRequired) {\n        tokeniser.error(`Missing second type argument in ${type} declaration`);\n      }\n    }\n\n    tokens.close =\n      tokeniser.consume(\">\") ||\n      tokeniser.error(`Missing greater-than sign \\`>\\` in ${type} declaration`);\n\n    if (tokeniser.probe(\"(\")) {\n      if (argumentAllowed) {\n        tokens.argsOpen = tokeniser.consume(\"(\");\n        ret.arguments.push(...argument_list(tokeniser));\n        tokens.argsClose =\n          tokeniser.consume(\")\") ||\n          tokeniser.error(\"Unterminated async iterable argument list\");\n      } else {\n        tokeniser.error(`Arguments are only allowed for \\`async iterable\\``);\n      }\n    }\n\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(`Missing semicolon after ${type} declaration`);\n\n    return ret.this;\n  }\n\n  get type() {\n    return this.tokens.base.value;\n  }\n  get readonly() {\n    return !!this.tokens.readonly;\n  }\n  get async() {\n    return !!this.tokens.async;\n  }\n\n  *validate(defs) {\n    for (const type of this.idlType) {\n      yield* type.validate(defs);\n    }\n    for (const argument of this.arguments) {\n      yield* argument.validate(defs);\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.readonly),\n        w.token(this.tokens.async),\n        w.token(this.tokens.base, w.ts.generic),\n        w.token(this.tokens.open),\n        w.ts.wrap(this.idlType.map((t) => t.write(w))),\n        w.token(this.tokens.close),\n        w.token(this.tokens.argsOpen),\n        w.ts.wrap(this.arguments.map((arg) => arg.write(w))),\n        w.token(this.tokens.argsClose),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this, parent: this.parent }\n    );\n  }\n}\n","import { Base } from \"./base.js\";\nimport { argument_list, autoParenter } from \"./helpers.js\";\n\nexport class Constructor extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    const base = tokeniser.consume(\"constructor\");\n    if (!base) {\n      return;\n    }\n    /** @type {Base[\"tokens\"]} */\n    const tokens = { base };\n    tokens.open =\n      tokeniser.consume(\"(\") ||\n      tokeniser.error(\"No argument list in constructor\");\n    const args = argument_list(tokeniser);\n    tokens.close =\n      tokeniser.consume(\")\") || tokeniser.error(\"Unterminated constructor\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"No semicolon after constructor\");\n    const ret = new Constructor({ source: tokeniser.source, tokens });\n    autoParenter(ret).arguments = args;\n    return ret;\n  }\n\n  get type() {\n    return \"constructor\";\n  }\n\n  *validate(defs) {\n    for (const argument of this.arguments) {\n      yield* argument.validate(defs);\n    }\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { parent } = this;\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.base, w.ts.nameless, { data: this, parent }),\n        w.token(this.tokens.open),\n        w.ts.wrap(this.arguments.map((arg) => arg.write(w))),\n        w.token(this.tokens.close),\n        w.token(this.tokens.termination),\n      ]),\n      { data: this, parent }\n    );\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { Constant } from \"./constant.js\";\nimport { IterableLike } from \"./iterable.js\";\nimport {\n  stringifier,\n  autofixAddExposedWindow,\n  getMemberIndentation,\n  getLastIndentation,\n  getFirstToken,\n  findLastIndex,\n  autoParenter,\n} from \"./helpers.js\";\nimport { validationError } from \"../error.js\";\nimport { checkInterfaceMemberDuplication } from \"../validators/interface.js\";\nimport { Constructor } from \"./constructor.js\";\nimport { Tokeniser } from \"../tokeniser.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\n\n/**\n * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n */\nfunction static_member(tokeniser) {\n  const special = tokeniser.consume(\"static\");\n  if (!special) return;\n  const member =\n    Attribute.parse(tokeniser, { special }) ||\n    Operation.parse(tokeniser, { special }) ||\n    tokeniser.error(\"No body in static member\");\n  return member;\n}\n\nexport class Interface extends Container {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, base, { partial = null } = {}) {\n    const tokens = { partial, base };\n    return Container.parse(\n      tokeniser,\n      new Interface({ source: tokeniser.source, tokens }),\n      {\n        inheritable: !partial,\n        allowedMembers: [\n          [Constant.parse],\n          [Constructor.parse],\n          [static_member],\n          [stringifier],\n          [IterableLike.parse],\n          [Attribute.parse],\n          [Operation.parse],\n        ],\n      }\n    );\n  }\n\n  get type() {\n    return \"interface\";\n  }\n\n  *validate(defs) {\n    yield* this.extAttrs.validate(defs);\n    if (\n      !this.partial &&\n      this.extAttrs.every((extAttr) => extAttr.name !== \"Exposed\")\n    ) {\n      const message = `Interfaces must have \\`[Exposed]\\` extended attribute. \\\nTo fix, add, for example, \\`[Exposed=Window]\\`. Please also consider carefully \\\nif your interface should also be exposed in a Worker scope. Refer to the \\\n[WebIDL spec section on Exposed](https://heycam.github.io/webidl/#Exposed) \\\nfor more information.`;\n      yield validationError(\n        this.tokens.name,\n        this,\n        \"require-exposed\",\n        message,\n        {\n          autofix: autofixAddExposedWindow(this),\n        }\n      );\n    }\n    const oldConstructors = this.extAttrs.filter(\n      (extAttr) => extAttr.name === \"Constructor\"\n    );\n    for (const constructor of oldConstructors) {\n      const message = `Constructors should now be represented as a \\`constructor()\\` operation on the interface \\\ninstead of \\`[Constructor]\\` extended attribute. Refer to the \\\n[WebIDL spec section on constructor operations](https://heycam.github.io/webidl/#idl-constructors) \\\nfor more information.`;\n      yield validationError(\n        constructor.tokens.name,\n        this,\n        \"constructor-member\",\n        message,\n        {\n          autofix: autofixConstructor(this, constructor),\n        }\n      );\n    }\n\n    const isGlobal = this.extAttrs.some((extAttr) => extAttr.name === \"Global\");\n    if (isGlobal) {\n      const factoryFunctions = this.extAttrs.filter(\n        (extAttr) => extAttr.name === \"LegacyFactoryFunction\"\n      );\n      for (const named of factoryFunctions) {\n        const message = `Interfaces marked as \\`[Global]\\` cannot have factory functions.`;\n        yield validationError(\n          named.tokens.name,\n          this,\n          \"no-constructible-global\",\n          message\n        );\n      }\n\n      const constructors = this.members.filter(\n        (member) => member.type === \"constructor\"\n      );\n      for (const named of constructors) {\n        const message = `Interfaces marked as \\`[Global]\\` cannot have constructors.`;\n        yield validationError(\n          named.tokens.base,\n          this,\n          \"no-constructible-global\",\n          message\n        );\n      }\n    }\n\n    yield* super.validate(defs);\n    if (!this.partial) {\n      yield* checkInterfaceMemberDuplication(defs, this);\n    }\n  }\n}\n\nfunction autofixConstructor(interfaceDef, constructorExtAttr) {\n  interfaceDef = autoParenter(interfaceDef);\n  return () => {\n    const indentation = getLastIndentation(\n      interfaceDef.extAttrs.tokens.open.trivia\n    );\n    const memberIndent = interfaceDef.members.length\n      ? getLastIndentation(getFirstToken(interfaceDef.members[0]).trivia)\n      : getMemberIndentation(indentation);\n    const constructorOp = Constructor.parse(\n      new Tokeniser(`\\n${memberIndent}constructor();`)\n    );\n    constructorOp.extAttrs = new ExtendedAttributes({\n      source: interfaceDef.source,\n      tokens: {},\n    });\n    autoParenter(constructorOp).arguments = constructorExtAttr.arguments;\n\n    const existingIndex = findLastIndex(\n      interfaceDef.members,\n      (m) => m.type === \"constructor\"\n    );\n    interfaceDef.members.splice(existingIndex + 1, 0, constructorOp);\n\n    const { close } = interfaceDef.tokens;\n    if (!close.trivia.includes(\"\\n\")) {\n      close.trivia += `\\n${indentation}`;\n    }\n\n    const { extAttrs } = interfaceDef;\n    const index = extAttrs.indexOf(constructorExtAttr);\n    const removed = extAttrs.splice(index, 1);\n    if (!extAttrs.length) {\n      extAttrs.tokens.open = extAttrs.tokens.close = undefined;\n    } else if (extAttrs.length === index) {\n      extAttrs[index - 1].tokens.separator = undefined;\n    } else if (!extAttrs[index].tokens.name.trivia.trim()) {\n      extAttrs[index].tokens.name.trivia = removed[0].tokens.name.trivia;\n    }\n  };\n}\n","import { validationError } from \"../error.js\";\n\n/**\n * @param {import(\"../validator.js\").Definitions} defs\n * @param {import(\"../productions/container.js\").Container} i\n */\nexport function* checkInterfaceMemberDuplication(defs, i) {\n  const opNames = groupOperationNames(i);\n  const partials = defs.partials.get(i.name) || [];\n  const mixins = defs.mixinMap.get(i.name) || [];\n  for (const ext of [...partials, ...mixins]) {\n    const additions = getOperations(ext);\n    const statics = additions.filter((a) => a.special === \"static\");\n    const nonstatics = additions.filter((a) => a.special !== \"static\");\n    yield* checkAdditions(statics, opNames.statics, ext, i);\n    yield* checkAdditions(nonstatics, opNames.nonstatics, ext, i);\n    statics.forEach((op) => opNames.statics.add(op.name));\n    nonstatics.forEach((op) => opNames.nonstatics.add(op.name));\n  }\n\n  /**\n   * @param {import(\"../productions/operation.js\").Operation[]} additions\n   * @param {Set<string>} existings\n   * @param {import(\"../productions/container.js\").Container} ext\n   * @param {import(\"../productions/container.js\").Container} base\n   */\n  function* checkAdditions(additions, existings, ext, base) {\n    for (const addition of additions) {\n      const { name } = addition;\n      if (name && existings.has(name)) {\n        const isStatic = addition.special === \"static\" ? \"static \" : \"\";\n        const message = `The ${isStatic}operation \"${name}\" has already been defined for the base interface \"${base.name}\" either in itself or in a mixin`;\n        yield validationError(\n          addition.tokens.name,\n          ext,\n          \"no-cross-overload\",\n          message\n        );\n      }\n    }\n  }\n\n  /**\n   * @param {import(\"../productions/container.js\").Container} i\n   * @returns {import(\"../productions/operation.js\").Operation[]}\n   */\n  function getOperations(i) {\n    return i.members.filter(({ type }) => type === \"operation\");\n  }\n\n  /**\n   * @param {import(\"../productions/container.js\").Container} i\n   */\n  function groupOperationNames(i) {\n    const ops = getOperations(i);\n    return {\n      statics: new Set(\n        ops.filter((op) => op.special === \"static\").map((op) => op.name)\n      ),\n      nonstatics: new Set(\n        ops.filter((op) => op.special !== \"static\").map((op) => op.name)\n      ),\n    };\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Constant } from \"./constant.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { stringifier } from \"./helpers.js\";\n\nexport class Mixin extends Container {\n  /**\n   * @typedef {import(\"../tokeniser.js\").Token} Token\n   *\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {Token} base\n   * @param {object} [options]\n   * @param {Token} [options.partial]\n   */\n  static parse(tokeniser, base, { partial } = {}) {\n    const tokens = { partial, base };\n    tokens.mixin = tokeniser.consume(\"mixin\");\n    if (!tokens.mixin) {\n      return;\n    }\n    return Container.parse(\n      tokeniser,\n      new Mixin({ source: tokeniser.source, tokens }),\n      {\n        allowedMembers: [\n          [Constant.parse],\n          [stringifier],\n          [Attribute.parse, { noInherit: true }],\n          [Operation.parse, { regular: true }],\n        ],\n      }\n    );\n  }\n\n  get type() {\n    return \"interface mixin\";\n  }\n}\n","import { Base } from \"./base.js\";\nimport {\n  unescape,\n  type_with_extended_attributes,\n  autoParenter,\n} from \"./helpers.js\";\nimport { ExtendedAttributes } from \"./extended-attributes.js\";\nimport { Default } from \"./default.js\";\n\nexport class Field extends Base {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser) {\n    /** @type {Base[\"tokens\"]} */\n    const tokens = {};\n    const ret = autoParenter(new Field({ source: tokeniser.source, tokens }));\n    ret.extAttrs = ExtendedAttributes.parse(tokeniser);\n    tokens.required = tokeniser.consume(\"required\");\n    ret.idlType =\n      type_with_extended_attributes(tokeniser, \"dictionary-type\") ||\n      tokeniser.error(\"Dictionary member lacks a type\");\n    tokens.name =\n      tokeniser.consumeKind(\"identifier\") ||\n      tokeniser.error(\"Dictionary member lacks a name\");\n    ret.default = Default.parse(tokeniser);\n    if (tokens.required && ret.default)\n      tokeniser.error(\"Required member must not have a default\");\n    tokens.termination =\n      tokeniser.consume(\";\") ||\n      tokeniser.error(\"Unterminated dictionary member, expected `;`\");\n    return ret.this;\n  }\n\n  get type() {\n    return \"field\";\n  }\n  get name() {\n    return unescape(this.tokens.name.value);\n  }\n  get required() {\n    return !!this.tokens.required;\n  }\n\n  *validate(defs) {\n    yield* this.idlType.validate(defs);\n  }\n\n  /** @param {import(\"../writer.js\").Writer} w */\n  write(w) {\n    const { parent } = this;\n    return w.ts.definition(\n      w.ts.wrap([\n        this.extAttrs.write(w),\n        w.token(this.tokens.required),\n        w.ts.type(this.idlType.write(w)),\n        w.name_token(this.tokens.name, { data: this, parent }),\n        this.default ? this.default.write(w) : \"\",\n        w.token(this.tokens.termination),\n      ]),\n      { data: this, parent }\n    );\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Field } from \"./field.js\";\n\nexport class Dictionary extends Container {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {object} [options]\n   * @param {import(\"../tokeniser.js\").Token} [options.partial]\n   */\n  static parse(tokeniser, { partial } = {}) {\n    const tokens = { partial };\n    tokens.base = tokeniser.consume(\"dictionary\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(\n      tokeniser,\n      new Dictionary({ source: tokeniser.source, tokens }),\n      {\n        inheritable: !partial,\n        allowedMembers: [[Field.parse]],\n      }\n    );\n  }\n\n  get type() {\n    return \"dictionary\";\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Attribute } from \"./attribute.js\";\nimport { Operation } from \"./operation.js\";\nimport { validationError } from \"../error.js\";\nimport { autofixAddExposedWindow } from \"./helpers.js\";\nimport { Constant } from \"./constant.js\";\n\nexport class Namespace extends Container {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   * @param {object} [options]\n   * @param {import(\"../tokeniser.js\").Token} [options.partial]\n   */\n  static parse(tokeniser, { partial } = {}) {\n    const tokens = { partial };\n    tokens.base = tokeniser.consume(\"namespace\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(\n      tokeniser,\n      new Namespace({ source: tokeniser.source, tokens }),\n      {\n        allowedMembers: [\n          [Attribute.parse, { noInherit: true, readonly: true }],\n          [Constant.parse],\n          [Operation.parse, { regular: true }],\n        ],\n      }\n    );\n  }\n\n  get type() {\n    return \"namespace\";\n  }\n\n  *validate(defs) {\n    if (\n      !this.partial &&\n      this.extAttrs.every((extAttr) => extAttr.name !== \"Exposed\")\n    ) {\n      const message = `Namespaces must have [Exposed] extended attribute. \\\nTo fix, add, for example, [Exposed=Window]. Please also consider carefully \\\nif your namespace should also be exposed in a Worker scope. Refer to the \\\n[WebIDL spec section on Exposed](https://heycam.github.io/webidl/#Exposed) \\\nfor more information.`;\n      yield validationError(\n        this.tokens.name,\n        this,\n        \"require-exposed\",\n        message,\n        {\n          autofix: autofixAddExposedWindow(this),\n        }\n      );\n    }\n    yield* super.validate(defs);\n  }\n}\n","import { Container } from \"./container.js\";\nimport { Operation } from \"./operation.js\";\nimport { Constant } from \"./constant.js\";\n\nexport class CallbackInterface extends Container {\n  /**\n   * @param {import(\"../tokeniser.js\").Tokeniser} tokeniser\n   */\n  static parse(tokeniser, callback, { partial = null } = {}) {\n    const tokens = { callback };\n    tokens.base = tokeniser.consume(\"interface\");\n    if (!tokens.base) {\n      return;\n    }\n    return Container.parse(\n      tokeniser,\n      new CallbackInterface({ source: tokeniser.source, tokens }),\n      {\n        inheritable: !partial,\n        allowedMembers: [\n          [Constant.parse],\n          [Operation.parse, { regular: true }],\n        ],\n      }\n    );\n  }\n\n  get type() {\n    return \"callback interface\";\n  }\n}\n","import { Tokeniser } from \"./tokeniser.js\";\nimport { Enum } from \"./productions/enum.js\";\nimport { Includes } from \"./productions/includes.js\";\nimport { ExtendedAttributes } from \"./productions/extended-attributes.js\";\nimport { Typedef } from \"./productions/typedef.js\";\nimport { CallbackFunction } from \"./productions/callback.js\";\nimport { Interface } from \"./productions/interface.js\";\nimport { Mixin } from \"./productions/mixin.js\";\nimport { Dictionary } from \"./productions/dictionary.js\";\nimport { Namespace } from \"./productions/namespace.js\";\nimport { CallbackInterface } from \"./productions/callback-interface.js\";\nimport { autoParenter } from \"./productions/helpers.js\";\nimport { Eof } from \"./productions/token.js\";\n\n/**\n * @param {Tokeniser} tokeniser\n * @param {object} options\n * @param {boolean} [options.concrete]\n * @param {Function[]} [options.productions]\n */\nfunction parseByTokens(tokeniser, options) {\n  const source = tokeniser.source;\n\n  function error(str) {\n    tokeniser.error(str);\n  }\n\n  function consume(...candidates) {\n    return tokeniser.consume(...candidates);\n  }\n\n  function callback() {\n    const callback = consume(\"callback\");\n    if (!callback) return;\n    if (tokeniser.probe(\"interface\")) {\n      return CallbackInterface.parse(tokeniser, callback);\n    }\n    return CallbackFunction.parse(tokeniser, callback);\n  }\n\n  function interface_(opts) {\n    const base = consume(\"interface\");\n    if (!base) return;\n    const ret =\n      Mixin.parse(tokeniser, base, opts) ||\n      Interface.parse(tokeniser, base, opts) ||\n      error(\"Interface has no proper body\");\n    return ret;\n  }\n\n  function partial() {\n    const partial = consume(\"partial\");\n    if (!partial) return;\n    return (\n      Dictionary.parse(tokeniser, { partial }) ||\n      interface_({ partial }) ||\n      Namespace.parse(tokeniser, { partial }) ||\n      error(\"Partial doesn't apply to anything\")\n    );\n  }\n\n  function definition() {\n    if (options.productions) {\n      for (const production of options.productions) {\n        const result = production(tokeniser);\n        if (result) {\n          return result;\n        }\n      }\n    }\n\n    return (\n      callback() ||\n      interface_() ||\n      partial() ||\n      Dictionary.parse(tokeniser) ||\n      Enum.parse(tokeniser) ||\n      Typedef.parse(tokeniser) ||\n      Includes.parse(tokeniser) ||\n      Namespace.parse(tokeniser)\n    );\n  }\n\n  function definitions() {\n    if (!source.length) return [];\n    const defs = [];\n    while (true) {\n      const ea = ExtendedAttributes.parse(tokeniser);\n      const def = definition();\n      if (!def) {\n        if (ea.length) error(\"Stray extended attributes\");\n        break;\n      }\n      autoParenter(def).extAttrs = ea;\n      defs.push(def);\n    }\n    const eof = Eof.parse(tokeniser);\n    if (options.concrete) {\n      defs.push(eof);\n    }\n    return defs;\n  }\n  const res = definitions();\n  if (tokeniser.position < source.length) error(\"Unrecognised tokens\");\n  return res;\n}\n\n/**\n * @param {string} str\n * @param {object} [options]\n * @param {*} [options.sourceName]\n * @param {boolean} [options.concrete]\n * @param {Function[]} [options.productions]\n * @return {import(\"./productions/base.js\").Base[]}\n */\nexport function parse(str, options = {}) {\n  const tokeniser = new Tokeniser(str);\n  if (typeof options.sourceName !== \"undefined\") {\n    // @ts-ignore (See Tokeniser.source in supplement.d.ts)\n    tokeniser.source.name = options.sourceName;\n  }\n  return parseByTokens(tokeniser, options);\n}\n","function noop(arg) {\n  return arg;\n}\n\nconst templates = {\n  wrap: (items) => items.join(\"\"),\n  trivia: noop,\n  name: noop,\n  reference: noop,\n  type: noop,\n  generic: noop,\n  nameless: noop,\n  inheritance: noop,\n  definition: noop,\n  extendedAttribute: noop,\n  extendedAttributeReference: noop,\n};\n\nexport class Writer {\n  constructor(ts) {\n    this.ts = Object.assign({}, templates, ts);\n  }\n\n  /**\n   * @param {string} raw\n   * @param {object} options\n   * @param {string} [options.unescaped]\n   * @param {import(\"./productions/base.js\").Base} [options.context]\n   * @returns\n   */\n  reference(raw, { unescaped, context }) {\n    if (!unescaped) {\n      unescaped = raw.startsWith(\"_\") ? raw.slice(1) : raw;\n    }\n    return this.ts.reference(raw, unescaped, context);\n  }\n\n  /**\n   * @param {import(\"./tokeniser.js\").Token} t\n   * @param {Function} wrapper\n   * @param {...any} args\n   * @returns\n   */\n  token(t, wrapper = noop, ...args) {\n    if (!t) {\n      return \"\";\n    }\n    const value = wrapper(t.value, ...args);\n    return this.ts.wrap([this.ts.trivia(t.trivia), value]);\n  }\n\n  reference_token(t, context) {\n    return this.token(t, this.reference.bind(this), { context });\n  }\n\n  name_token(t, arg) {\n    return this.token(t, this.ts.name, arg);\n  }\n\n  identifier(id, context) {\n    return this.ts.wrap([\n      this.reference_token(id.tokens.value, context),\n      this.token(id.tokens.separator),\n    ]);\n  }\n}\n\nexport function write(ast, { templates: ts = templates } = {}) {\n  ts = Object.assign({}, templates, ts);\n\n  const w = new Writer(ts);\n\n  return ts.wrap(ast.map((it) => it.write(w)));\n}\n","import { validationError as error } from \"./error.js\";\n\nfunction getMixinMap(all, unique) {\n  const map = new Map();\n  const includes = all.filter((def) => def.type === \"includes\");\n  for (const include of includes) {\n    const mixin = unique.get(include.includes);\n    if (!mixin) {\n      continue;\n    }\n    const array = map.get(include.target);\n    if (array) {\n      array.push(mixin);\n    } else {\n      map.set(include.target, [mixin]);\n    }\n  }\n  return map;\n}\n\n/**\n * @typedef {ReturnType<typeof groupDefinitions>} Definitions\n */\nfunction groupDefinitions(all) {\n  const unique = new Map();\n  const duplicates = new Set();\n  const partials = new Map();\n  for (const def of all) {\n    if (def.partial) {\n      const array = partials.get(def.name);\n      if (array) {\n        array.push(def);\n      } else {\n        partials.set(def.name, [def]);\n      }\n      continue;\n    }\n    if (!def.name) {\n      continue;\n    }\n    if (!unique.has(def.name)) {\n      unique.set(def.name, def);\n    } else {\n      duplicates.add(def);\n    }\n  }\n  return {\n    all,\n    unique,\n    partials,\n    duplicates,\n    mixinMap: getMixinMap(all, unique),\n    cache: {\n      typedefIncludesDictionary: new WeakMap(),\n      dictionaryIncludesRequiredField: new WeakMap(),\n    },\n  };\n}\n\nfunction* checkDuplicatedNames({ unique, duplicates }) {\n  for (const dup of duplicates) {\n    const { name } = dup;\n    const message = `The name \"${name}\" of type \"${\n      unique.get(name).type\n    }\" was already seen`;\n    yield error(dup.tokens.name, dup, \"no-duplicate\", message);\n  }\n}\n\nfunction* validateIterable(ast) {\n  const defs = groupDefinitions(ast);\n  for (const def of defs.all) {\n    if (def.validate) {\n      yield* def.validate(defs);\n    }\n  }\n  yield* checkDuplicatedNames(defs);\n}\n\n// Remove this once all of our support targets expose `.flat()` by default\nfunction flatten(array) {\n  if (array.flat) {\n    return array.flat();\n  }\n  return [].concat(...array);\n}\n\n/**\n * @param {import(\"./productions/base.js\").Base[]} ast\n * @return {import(\"./error.js\").WebIDLErrorData[]} validation errors\n */\nexport function validate(ast) {\n  return [...validateIterable(flatten(ast))];\n}\n"],"names":["root","factory","exports","module","define","amd","globalThis","__webpack_require__","definition","key","o","Object","defineProperty","enumerable","get","obj","prop","prototype","hasOwnProperty","call","Symbol","toStringTag","value","error","source","position","current","message","kind","level","autofix","ruleName","sliceTokens","count","slice","Math","max","tokensToText","inputs","precedes","text","map","t","trivia","join","nextToken","type","length","line","precedingLastLine","splitted","split","lastLine","subsequentTokens","subsequentText","sourceContext","repeat","contextType","context","name","partial","node","hierarchy","parent","unshift","n","base","target","result","appendIfExist","contextAsText","bareMessage","sourceName","input","tokens","syntaxError","validationError","token","options","index","Base","constructor","defineProperties","this","writable","toJSON","json","undefined","inheritance","proto","descMap","getOwnPropertyDescriptors","entries","getPrototypeOf","idlTypeIncludesDictionary","idlType","defs","useNullableInner","union","def","unique","typedefIncludesDictionary","cache","has","set","reference","dictionary","nullable","subtype","dictionaryIncludesRequiredField","dict","members","some","field","required","superdict","ArrayBase","Array","super","WrappedToken","tokeniser","consumeKind","write","w","ts","wrap","separator","Eof","tokenName","list","parser","listName","extAttrValueSyntax","renamedLegacies","Map","extAttrListItems","syntax","toks","ExtendedAttributeParameters","assign","consume","ret","autoParenter","asterisk","secondaryName","open","rhsIsList","argument_list","close","rhsType","reference_token","p","identifier","SimpleExtendedAttribute","params","parse","extAttr","arg","arguments","validate","extendedAttribute","extendedAttributeReference","ExtendedAttributes","push","unconsume","probe","ea","type_suffix","single_type","typeName","Type","return_type","type_with_extended_attributes","keyType","stringTypes","keyIdlType","valueType","generic_type","primitive_type","typeNameKeywords","generic","typ","or","union_type","extAttrs","Boolean","prefix","postfix","filter","typedef","targetToken","firstToken","ref","unescaped","type_body","Default","const_value","expression","const_data","negative","Argument","start_position","optional","variadic","argumentNameKeywords","default","autofixOptionalDictionaryDefaultValue","indexOf","a","isLastRequiredArgument","getFirstToken","name_token","data","Tokeniser","Operation","special","regular","termination","includes","argument","body","nameless","Attribute","noInherit","readonly","startsWith","allowDangler","first","items","item","num_type","integer_type","decimal_type","voidToken","stringifier","getLastIndentation","str","lines","match","autofixAddExposedWindow","exposed","existing","test","values","sort","x","y","Proxy","isArray","tokenRe","decimal","integer","string","whitespace","comment","other","nonRegexTerminals","concat","punctuations","reserved","idl","lastCharIndex","nextChar","charAt","attemptTokenMatch","noFlushTrivia","currentTrivia","pop","lastIndex","WebIDLParseError","punctuation","Error","re","exec","tokenise","probeKind","candidates","consumeIdentifier","EnumValue","Enum","v","Includes","mixin","Typedef","CallbackFunction","Container","instance","inheritable","allowedMembers","colon","mem","args","member","callback","m","Constant","IterableLike","async","secondTypeRequired","secondTypeAllowed","argumentAllowed","argsOpen","argsClose","Constructor","static_member","Interface","every","oldConstructors","autofixConstructor","factoryFunctions","named","constructors","i","opNames","ops","getOperations","statics","Set","op","nonstatics","groupOperationNames","partials","mixins","mixinMap","ext","additions","checkAdditions","forEach","add","existings","addition","checkInterfaceMemberDuplication","interfaceDef","constructorExtAttr","indentation","memberIndent","parentTrivia","indentCh","getMemberIndentation","constructorOp","existingIndex","array","predicate","reverse","findIndex","findLastIndex","splice","removed","trim","Mixin","Field","Dictionary","Namespace","CallbackInterface","parseByTokens","interface_","opts","productions","production","res","eof","concrete","definitions","noop","templates","Writer","raw","wrapper","bind","id","ast","it","getMixinMap","all","include","validateIterable","duplicates","WeakMap","groupDefinitions","dup","checkDuplicatedNames","flat"],"sourceRoot":""}